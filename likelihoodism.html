<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Likelihoodism | Modeling Mindsets</title>
  <meta name="description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Likelihoodism | Modeling Mindsets" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Likelihoodism | Modeling Mindsets" />
  
  <meta name="twitter:description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2022-06-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-inference.html"/>
<link rel="next" href="causal-inference.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>

<style>

#cta-button-desktop:hover, #cta-button-device:hover {
  background-color:   #ffc266; 
  border-color:   #ffc266; 
  box-shadow: none;
}
#cta-button-desktop, #cta-button-device{
  color: white;
  background-color:  #ffa31a;
  text-shadow:1px 1px 0 #444;
  text-decoration: none;
  border: 2px solid  #ffa31a;
  border-radius: 10px;
  position: fixed;
  padding: 5px 10px;
  z-index: 10;
  }

#cta-button-device {
  box-shadow: 0px 10px 10px -5px rgba(194,180,190,1);
  display:none;
  right: 20px;
  bottom: 20px;
  font-size: 20px;
 }

#cta-button-desktop {
  box-shadow: 0px 20px 20px -10px rgba(194,180,190,1);
  display:display;
  padding: 8px 16px;
  right: 40px;
  bottom: 40px;
  font-size: 25px;
}

@media (max-width : 450px) {
  #cta-button-device {display:block;}
  #cta-button-desktop {display:none;}
}


</style>





<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Mindsets</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="what-this-book-is-about.html"><a href="what-this-book-is-about.html"><i class="fa fa-check"></i><b>1</b> What This Book is About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="what-this-book-is-about.html"><a href="what-this-book-is-about.html#who-this-book-is-for"><i class="fa fa-check"></i><b>1.1</b> Who This Book is For</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Models</a></li>
<li class="chapter" data-level="3" data-path="mindsets.html"><a href="mindsets.html"><i class="fa fa-check"></i><b>3</b> Mindsets</a></li>
<li class="chapter" data-level="4" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>4</b> Statistical Modeling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#random-variables"><i class="fa fa-check"></i><b>4.1</b> Random Variables</a></li>
<li class="chapter" data-level="4.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#probability-distributions"><i class="fa fa-check"></i><b>4.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="4.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#assuming-a-distribution"><i class="fa fa-check"></i><b>4.3</b> Assuming a Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-model"><i class="fa fa-check"></i><b>4.4</b> Statistical Model</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i>Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-hypothesis"><i class="fa fa-check"></i>Statistical Hypothesis</a></li>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#interpreting-parameters"><i class="fa fa-check"></i>Interpreting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="statistical-modeling.html"><a href="statistical-modeling.html#joint-or-conditional-distribution"><i class="fa fa-check"></i><b>4.5</b> Joint or Conditional Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="statistical-modeling.html"><a href="statistical-modeling.html#regression-models"><i class="fa fa-check"></i><b>4.6</b> Regression Models</a></li>
<li class="chapter" data-level="4.7" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-evaluation"><i class="fa fa-check"></i><b>4.7</b> Model Evaluation</a></li>
<li class="chapter" data-level="4.8" data-path="statistical-modeling.html"><a href="statistical-modeling.html#data-generating-process-dgp"><i class="fa fa-check"></i><b>4.8</b> Data-Generating Process (DGP)</a></li>
<li class="chapter" data-level="4.9" data-path="statistical-modeling.html"><a href="statistical-modeling.html#drawing-conclusions-about-the-world"><i class="fa fa-check"></i><b>4.9</b> Drawing Conclusions About the World</a></li>
<li class="chapter" data-level="4.10" data-path="statistical-modeling.html"><a href="statistical-modeling.html#strengths"><i class="fa fa-check"></i><b>4.10</b> Strengths</a></li>
<li class="chapter" data-level="4.11" data-path="statistical-modeling.html"><a href="statistical-modeling.html#limitations"><i class="fa fa-check"></i><b>4.11</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="frequentist-inference.html"><a href="frequentist-inference.html"><i class="fa fa-check"></i><b>5</b> Frequentist Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="frequentist-inference.html"><a href="frequentist-inference.html#frequentist-probability"><i class="fa fa-check"></i><b>5.1</b> Frequentist probability</a></li>
<li class="chapter" data-level="5.2" data-path="frequentist-inference.html"><a href="frequentist-inference.html#estimators-are-random-variables"><i class="fa fa-check"></i><b>5.2</b> Estimators are Random Variables</a></li>
<li class="chapter" data-level="5.3" data-path="frequentist-inference.html"><a href="frequentist-inference.html#null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>5.3</b> Null Hypothesis Significance Testing</a></li>
<li class="chapter" data-level="5.4" data-path="frequentist-inference.html"><a href="frequentist-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.4</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.5" data-path="frequentist-inference.html"><a href="frequentist-inference.html#strengths-1"><i class="fa fa-check"></i><b>5.5</b> Strengths</a></li>
<li class="chapter" data-level="5.6" data-path="frequentist-inference.html"><a href="frequentist-inference.html#limitations-1"><i class="fa fa-check"></i><b>5.6</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>6</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayes-theorem"><i class="fa fa-check"></i><b>6.1</b> Bayes Theorem</a></li>
<li class="chapter" data-level="6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prior-probability"><i class="fa fa-check"></i><b>6.2</b> Prior Probability</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#picking-a-prior"><i class="fa fa-check"></i>Picking a Prior</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#likelihood"><i class="fa fa-check"></i><b>6.3</b> Likelihood</a></li>
<li class="chapter" data-level="6.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#evidence"><i class="fa fa-check"></i><b>6.4</b> Evidence</a></li>
<li class="chapter" data-level="6.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-probability-estimation"><i class="fa fa-check"></i><b>6.5</b> Posterior Probability Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#sample-from-the-posterior-with-mcmc"><i class="fa fa-check"></i>Sample From the Posterior with MCMC</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summarizing-the-posterior-samples"><i class="fa fa-check"></i><b>6.6</b> Summarizing the Posterior Samples</a></li>
<li class="chapter" data-level="6.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#from-model-to-world"><i class="fa fa-check"></i><b>6.7</b> From Model to World</a></li>
<li class="chapter" data-level="6.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#simulate-to-predict"><i class="fa fa-check"></i><b>6.8</b> Simulate to Predict</a></li>
<li class="chapter" data-level="6.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#strengths-2"><i class="fa fa-check"></i><b>6.9</b> Strengths</a></li>
<li class="chapter" data-level="6.10" data-path="bayesian-inference.html"><a href="bayesian-inference.html#limitations-2"><i class="fa fa-check"></i><b>6.10</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="likelihoodism.html"><a href="likelihoodism.html"><i class="fa fa-check"></i><b>7</b> Likelihoodism</a>
<ul>
<li class="chapter" data-level="7.1" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-principle"><i class="fa fa-check"></i><b>7.1</b> Likelihood Principle</a></li>
<li class="chapter" data-level="7.2" data-path="likelihoodism.html"><a href="likelihoodism.html#law-of-likelihood"><i class="fa fa-check"></i><b>7.2</b> Law of Likelihood</a></li>
<li class="chapter" data-level="7.3" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-intervals"><i class="fa fa-check"></i><b>7.3</b> Likelihood Intervals</a></li>
<li class="chapter" data-level="7.4" data-path="likelihoodism.html"><a href="likelihoodism.html#why-frequentism-violates-the-likelihood-principle"><i class="fa fa-check"></i><b>7.4</b> Why Frequentism Violates the Likelihood Principle</a></li>
<li class="chapter" data-level="7.5" data-path="likelihoodism.html"><a href="likelihoodism.html#strengths-3"><i class="fa fa-check"></i><b>7.5</b> Strengths</a></li>
<li class="chapter" data-level="7.6" data-path="likelihoodism.html"><a href="likelihoodism.html#limitations-3"><i class="fa fa-check"></i><b>7.6</b> Limitations</a></li>
<li class="chapter" data-level="7.7" data-path="likelihoodism.html"><a href="likelihoodism.html#resources"><i class="fa fa-check"></i><b>7.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>8</b> Causal inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="causal-inference.html"><a href="causal-inference.html#does-the-drug-help"><i class="fa fa-check"></i><b>8.1</b> Does The Drug Help?</a></li>
<li class="chapter" data-level="8.2" data-path="causal-inference.html"><a href="causal-inference.html#causality"><i class="fa fa-check"></i><b>8.2</b> Causality</a></li>
<li class="chapter" data-level="8.3" data-path="causal-inference.html"><a href="causal-inference.html#the-causal-mindset"><i class="fa fa-check"></i><b>8.3</b> The Causal Mindset</a></li>
<li class="chapter" data-level="8.4" data-path="causal-inference.html"><a href="causal-inference.html#directed-acyclical-graphs"><i class="fa fa-check"></i><b>8.4</b> Directed Acyclical Graphs</a></li>
<li class="chapter" data-level="8.5" data-path="causal-inference.html"><a href="causal-inference.html#many-frameworks-for-causality"><i class="fa fa-check"></i><b>8.5</b> Many Frameworks For Causality</a></li>
<li class="chapter" data-level="8.6" data-path="causal-inference.html"><a href="causal-inference.html#from-causal-to-conditional"><i class="fa fa-check"></i><b>8.6</b> From Causal to Conditional</a></li>
<li class="chapter" data-level="8.7" data-path="causal-inference.html"><a href="causal-inference.html#strengths-4"><i class="fa fa-check"></i><b>8.7</b> Strengths</a></li>
<li class="chapter" data-level="8.8" data-path="causal-inference.html"><a href="causal-inference.html#limitations-4"><i class="fa fa-check"></i><b>8.8</b> Limitations</a></li>
<li class="chapter" data-level="8.9" data-path="causal-inference.html"><a href="causal-inference.html#further-reading"><i class="fa fa-check"></i><b>8.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>9</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="machine-learning.html"><a href="machine-learning.html#one-or-many-mindsets"><i class="fa fa-check"></i><b>9.1</b> One or Many Mindsets?</a></li>
<li class="chapter" data-level="9.2" data-path="machine-learning.html"><a href="machine-learning.html#computer-oriented-task-driven-and-externally-motivated"><i class="fa fa-check"></i><b>9.2</b> Computer-Oriented, Task-Driven and Externally Motivated</a></li>
<li class="chapter" data-level="9.3" data-path="machine-learning.html"><a href="machine-learning.html#strengths-5"><i class="fa fa-check"></i><b>9.3</b> Strengths</a></li>
<li class="chapter" data-level="9.4" data-path="machine-learning.html"><a href="machine-learning.html#limitations-5"><i class="fa fa-check"></i><b>9.4</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised-ml.html"><a href="supervised-ml.html"><i class="fa fa-check"></i><b>10</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="supervised-ml.html"><a href="supervised-ml.html#competing-with-the-wrong-mindset"><i class="fa fa-check"></i><b>10.1</b> Competing With the Wrong Mindset</a></li>
<li class="chapter" data-level="10.2" data-path="supervised-ml.html"><a href="supervised-ml.html#predict-everything"><i class="fa fa-check"></i><b>10.2</b> Predict Everything</a></li>
<li class="chapter" data-level="10.3" data-path="supervised-ml.html"><a href="supervised-ml.html#supervised-machine-learning"><i class="fa fa-check"></i><b>10.3</b> Supervised Machine Learning</a></li>
<li class="chapter" data-level="10.4" data-path="supervised-ml.html"><a href="supervised-ml.html#learning-is-searching"><i class="fa fa-check"></i><b>10.4</b> Learning Is Searching</a></li>
<li class="chapter" data-level="10.5" data-path="supervised-ml.html"><a href="supervised-ml.html#overfitting"><i class="fa fa-check"></i><b>10.5</b> Overfitting</a></li>
<li class="chapter" data-level="10.6" data-path="supervised-ml.html"><a href="supervised-ml.html#evaluation"><i class="fa fa-check"></i><b>10.6</b> Evaluation</a></li>
<li class="chapter" data-level="10.7" data-path="supervised-ml.html"><a href="supervised-ml.html#an-automatable-mindset"><i class="fa fa-check"></i><b>10.7</b> An Automatable Mindset</a></li>
<li class="chapter" data-level="10.8" data-path="supervised-ml.html"><a href="supervised-ml.html#a-competitive-mindset"><i class="fa fa-check"></i><b>10.8</b> A Competitive Mindset</a></li>
<li class="chapter" data-level="10.9" data-path="supervised-ml.html"><a href="supervised-ml.html#nature-statistics-and-supervised-learning"><i class="fa fa-check"></i><b>10.9</b> Nature, Statistics and Supervised Learning</a></li>
<li class="chapter" data-level="10.10" data-path="supervised-ml.html"><a href="supervised-ml.html#strengths-6"><i class="fa fa-check"></i><b>10.10</b> Strengths</a></li>
<li class="chapter" data-level="10.11" data-path="supervised-ml.html"><a href="supervised-ml.html#limitations-6"><i class="fa fa-check"></i><b>10.11</b> Limitations</a></li>
<li class="chapter" data-level="10.12" data-path="supervised-ml.html"><a href="supervised-ml.html#references"><i class="fa fa-check"></i><b>10.12</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#what-type-of-traveler-are-you"><i class="fa fa-check"></i><b>11.1</b> What Type of Traveler Are You?</a></li>
<li class="chapter" data-level="11.2" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#the-unsupervised-learning-mindset"><i class="fa fa-check"></i><b>11.2</b> The Unsupervised Learning Mindset</a></li>
<li class="chapter" data-level="11.3" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#many-tasks"><i class="fa fa-check"></i><b>11.3</b> Many Tasks</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#clustering-and-outlier-detection"><i class="fa fa-check"></i><b>11.3.1</b> Clustering and Outlier Detection</a></li>
<li class="chapter" data-level="11.3.2" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#anomaly-detection"><i class="fa fa-check"></i><b>11.3.2</b> Anomaly Detection</a></li>
<li class="chapter" data-level="11.3.3" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#association-rule-learning"><i class="fa fa-check"></i><b>11.3.3</b> Association Rule Learning</a></li>
<li class="chapter" data-level="11.3.4" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#dimensionality-reduction"><i class="fa fa-check"></i><b>11.3.4</b> Dimensionality Reduction</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#strengths-7"><i class="fa fa-check"></i><b>11.4</b> Strengths</a></li>
<li class="chapter" data-level="11.5" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#limitations-7"><i class="fa fa-check"></i><b>11.5</b> Limitations</a></li>
<li class="chapter" data-level="11.6" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#resources-1"><i class="fa fa-check"></i><b>11.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>12</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="13" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>13</b> Deep Learning</a></li>
<li class="chapter" data-level="14" data-path="interpretable-ml.html"><a href="interpretable-ml.html"><i class="fa fa-check"></i><b>14</b> Interpretable Machine Learning</a></li>
<li class="chapter" data-level="15" data-path="design-based-inference.html"><a href="design-based-inference.html"><i class="fa fa-check"></i><b>15</b> Design-based Inference</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li> 
<li><a href="https://christophmolnar.com/impressum/" target="_blank">Impressum</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling Mindsets</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">

<a id="cta-button-desktop" href="https://leanpub.com/modeling-mindsets" rel="noopener noreferrer" target="blank"> Buy Book </a>

<a id="cta-button-device" href="https://leanpub.com/modeling-mindsets" rel="noopener noreferrer" target="blank">Buy</a>
  

<div id="likelihoodism" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Likelihoodism<a href="likelihoodism.html#likelihoodism" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li>The likelihood function is all you need and is interpreted as evidence for a statistical hypothesis (law of likelihood)</li>
<li>Statistical hypotheses are compared by the ratio of their likelihoods.</li>
<li>A <a href="statistical-modeling.html#statistical-modeling">statistical modeling mindset</a> with <a href="#frequent-inference">frequentism</a> and <a href="bayesian-inference.html#bayesian-inference">Bayesianism</a> as alternatives.</li>
</ul>
<!-- metaphor -->
<p>A frequentist, a Bayesian, and a likelihoodist walk into a bar, a wine bar.
The sommelier quickly joins the three.
The Bayesian wants to hear the sommelier’s opinion first before trying the wines..
The frequentist asks the sommelier about the tasting process: is the number of wines fixed in advance? Is the tasting over when the customer has found a suitable wine? How are subsequent wines selected?
The likelihoodist politely tells the sommelier to fuck off.</p>
<!-- problem with frequentism -->
<p><a href="frequentist-inference.html#frequentist-inference">Frequentist inference</a> has a long list of limitations.
But it’s still the dominant statistical mindset in science and elsewhere.
Bayesian analysis has seen a resurgence thanks to increased computational power for sampling from posteriors with MCMC.
But using subjective prior probabilities doesn’t sit well with many statisticians.
Could there be another way to “reform” the frequentist mindset?
A mindset without the flawed hypothesis testing and without priors?</p>
<p>Welcome to the <strong>likelihoodist mindset.</strong></p>
<!-- underdog -->
<p>I studied statistics for 5 years, worked as a statistician and data scientist for 3 years, and then did PhD studies in machine learning for 4.5 years.
In those 12 years of statistics, I never learned anything about likelihoodism.
It’s fair to say that likelihoodism is the underdog.
Likelihoodism leads a shadowy existence while Bayesianism and frequentism are engaged in an epic battle.</p>
<!-- why learn about it -->
<p>Likelihoodism is the purist among the statistical modeling mindsets.
A mindset that focuses entirely on the likelihood function.
Likelihoodism is an attempt to make statistics as objective as possible.</p>
<!-- short reminder: likelihood -->
<p><strong>All three mindsets use likelihood functions in different ways.</strong>
A quick recap: The likelihood function is the same as the data density function, but the roles of data and parameters are reversed.
Data <span class="math inline">\(X\)</span> are “fixed” and the likelihood is a function of the parameters <span class="math inline">\(\theta\)</span> <span class="math inline">\(P(\theta; X) = P(X = x | \theta)\)</span>.
The likelihood links observed data to theoretic distributions.
<!-- use of the likelihood function -->
Bayesians multiply prior distributions with the likelihood to get the posterior distributions of the parameters.
Frequentists use the likelihood to estimate parameters and construct “imagined” experiments that teach us about long-run frequencies (hypothesis tests and confidence intervals).
Likelihoodists view the likelihood as evidence derived from data for a statistical hypothesis.
Likelihoodists emphasize the likelihood and reject the non-likelihood elements from frequentism and Bayesianism:
Likelihoodists reject priors because they are subjective;
Likelihoodists reject the frequentists’ reliance on “imagined” experiments because these never-observed experiments violate the likelihood principle.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:three-stat-mindsets"></span>
<img src="figures/three-stat-mindsets-1.png" alt="How Bayesianism, frequentism, and likelihoodism overlap and differ. Figure inspired by Greg Gandenberger: https://gandenberger.org/2014/07/28/intro-to-statistical-methods-2/." width="\textwidth" />
<p class="caption">
FIGURE 7.1: How Bayesianism, frequentism, and likelihoodism overlap and differ. Figure inspired by Greg Gandenberger: <a href="https://gandenberger.org/2014/07/28/intro-to-statistical-methods-2/" class="uri">https://gandenberger.org/2014/07/28/intro-to-statistical-methods-2/</a>.
</p>
</div>
<!-- likelihood principle as foundation -->
<p>But what is the likelihood principle that is so central to likelihoodism?</p>
<div id="likelihood-principle" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Likelihood Principle<a href="likelihoodism.html#likelihood-principle" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>“The likelihood principle asserts that two observations that generate identical likelihood functions are equivalent as evidence.”<span class="citation"><sup><a href="#ref-richard2017statistical" role="doc-biblioref">10</a></sup></span>
As a consequence, all evidence that comes from the data about a quantity of interest <span class="math inline">\(\theta\)</span> has to be part of the likelihood function <span class="math inline">\(P(\theta;X)\)</span>.
If we reverse the statement: If information from the data influences the analysis but is not part of the likelihood, then the likelihood principle is violated.</p>
<p>Let’s say we want to estimate the average waiting time for a public bus.
To model waiting times, the exponential distribution is a good choice.
So we could assume that <span class="math inline">\(X \sim Exp(\lambda)\)</span>, where <span class="math inline">\(\lambda\)</span> can be interpreted as the inverse waiting time.
The expected waiting time is <span class="math inline">\(\frac{1}{\lambda}\)</span>.
We have collected <span class="math inline">\(n\)</span> waiting times <span class="math inline">\(x_1, \ldots, x_n\)</span>.
The likelihood function is:</p>
<p><span class="math display">\[P(\lambda; x_1, \ldots, x_n) = \lambda^n exp\left(-\lambda \sum_{i=n}^n x^{(i)}\right).\]</span></p>
<p>In all three mindsets, we could work with this likelihood.
Bayesians would, in addition, assume a prior distribution for <span class="math inline">\(\lambda\)</span>.
Whether the likelihood principle is violated depends on what we do after calculating the likelihood.
Bayesians obtain a posterior distribution for <span class="math inline">\(\lambda\)</span> that is interpreted as belief about the parameter.
The likelihoodist might report the likelihood region for <span class="math inline">\(\lambda\)</span>, which can be interpreted as relative evidence for a range of parameter values compared to the maximum likelihood estimate for <span class="math inline">\(\lambda\)</span>.
Both the Bayesian and the likelihoodist approaches adhere to the likelihood principle:
All evidence from the data about <span class="math inline">\(\lambda\)</span> is included in the likelihood.
Bayesians use priors, but as long as they don’t include any information from the data, it’s fine. <a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>The frequentist might test whether <span class="math inline">\(\lambda\)</span> is significantly smaller than a certain value.
When performing such a test, the frequentist has to “imagine” experiments under the null hypothesis distribution.
But the null hypothesis is not part of the likelihood.
Frequentists choose the distribution under the null hypothesis based on how they “imagine” repetitions of the sample or experiment.
This, in turn, depends on how the experiment was conducted or how the data were collected in the first place.
You will see later an example of a coin toss where the same data from different experiments lead to different conclusions in the frequentist mindset.</p>
<!--
The likelihood principle is a consequence of 1) the sufficiency principle and 2) the conditionality principle.[@birnbaum1962foundations]
The sufficiency principle says that a sufficient statistic $S(X)$ of a quantity of interest summarizes all relevant evidence from the data.
The conditionality principles says that experiments or samples that were not performed should be ignored.
Many frequentist approaches directly violate the conditionality principle.
Statistical hypothesis tests are designed around experiments that never happen.
-->
<p>So the big difference between frequentism and likelihoodism is the likelihood principle.
The likelihood principle gives the likelihood function the monopoly over data evidence.
But the likelihood principle alone is not sufficient to create a coherent modeling mindset.
We need the law of likelihood.</p>
</div>
<div id="law-of-likelihood" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Law of Likelihood<a href="likelihoodism.html#law-of-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The law of likelihood is the foundation for using the likelihood function alone formaking modeling decisions.
The law of likelihood says<span class="citation"><sup><a href="#ref-hacking1965logic" role="doc-biblioref">11</a></sup></span>:</p>
<ul>
<li>Given:
<ul>
<li>Hypotheses <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span>; data <span class="math inline">\(\mathbf{x} = \{x^{(1)}, \dots, x^{(n)}\}\)</span>.</li>
<li>Likelihood for <span class="math inline">\(H_1\)</span> is larger than for <span class="math inline">\(H_2\)</span>: <span class="math inline">\(P_{H_1}(X = \mathbf{x}) &gt; P_{H_2}(X = \mathbf{x})\)</span>.</li>
</ul></li>
<li>Then:
<ul>
<li>Observation <span class="math inline">\(X=\mathbf{x}\)</span> is evidence supporting <span class="math inline">\(H_1\)</span> over <span class="math inline">\(H_2\)</span>.</li>
<li>The likelihood ratio <span class="math inline">\(P_{H_1}(x)/P_{H_2}(x)\)</span> measures the strength of this evidence.</li>
</ul></li>
</ul>
<p>The hypotheses can be the same statistical model, but with different parameter values <span class="math inline">\(\theta\)</span>.
Returning to the bus waiting time example, <span class="math inline">\(H_1\)</span> could be that <span class="math inline">\(\lambda = 1\)</span>, and <span class="math inline">\(H_2\)</span> could be that <span class="math inline">\(\lambda = 0.5\)</span>.
The resulting likelihood ratio might be:</p>
<p><span class="math display">\[P(\lambda = 1;x_1, \ldots, x_n)/P(\lambda = 0.5;x_1, \ldots, x_n) = 4\]</span>.</p>
<p>The likelihood ratio is the likelihood of one statistical hypothesis divided by the likelihood of another.
As a reminder, statistical hypotheses are statistical models where, optionally, some or all of the parameters are assigned by hand rather than learned from the data.
The law of likelihood tells us, that to compare hypotheses <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span> with their likelihood ratio:</p>
<p><span class="math display">\[\frac{P(H_1;X = \mathbf{x})}{P(H_2;X = \mathbf{x})}\]</span></p>
<p>In frequentism, likelihood ratios are often used as test statistics for hypothesis tests.
In likelihoodism, the likelihood ratio is interpreted as evidence.</p>
<p>Likelihoodists may use a rule of thumb for judging the strength of evidence.
For example, a likelihood ratio of 8 is considered fairly strong and 32 or more is considered “strong favoring”.<span class="citation"><sup><a href="#ref-richard2017statistical" role="doc-biblioref">10</a></sup></span>
In our example, a likelihood ratio of 4 in favor of <span class="math inline">\(H_1: \lambda = 1\)</span> over <span class="math inline">\(H_2: \lambda = 0.5\)</span> is not enough to be “fairly strong”.
<span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span> can also be more complex hypotheses, such as regression models with different covariates or assumptions.</p>
<!-- stronger law -->
<p><strong>The law of likelihood is stronger than the likelihood principle:</strong>
The likelihood principle states that the all evidence from the data must be in the likelihood;
<strong>The law of likelihood describes how evidence can be quantified and compared.</strong>
And this is where Bayesian inference and likelihoodism differ.
Bayesians are not guided by the law of likelihood, but by Bayesian updating and a subjective interpretation of interpretability.</p>
<!-- clear vision -->
<p>The law of likelihood makes it clear how we can compare statistical hypotheses:
Not by hypothesis testing, but by their likelihood ratios.
The larger the ratio, the stronger the evidence for one hypothesis over another.</p>
<!-- shortcomings -->
<p>This is also where likelihoodism reaches a dead end.
The ratio may only be interpreted as evidential favoring.
The likelihoodist mindset doesn’t come with guidance on what we should believe about the parameters or what decision/action to take based on the results.
The likelihood ratio only tell us which hypothesis is favored.</p>
</div>
<div id="likelihood-intervals" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Likelihood Intervals<a href="likelihoodism.html#likelihood-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- using likelihood intervals -->
<p>Likelihood intervals are the likelihoodist analogue to frequentist confidence interval and Bayesian credible intervals.
Likelihood intervals are interpreted in terms of, you guessed it, relative likelihood.
The likelihood interval of a model parameter <span class="math inline">\(\theta\)</span> is the set of all <span class="math inline">\(\theta\)</span> values that yield a relative likelihood greater than a certain threshold:</p>
<p><span class="math display">\[\left\{\theta: \frac{L(\theta| X)}{L(\hat{\theta}| X)} \geq \frac{p}{100}\right\}\]</span></p>
<p>The <span class="math inline">\(\hat{\theta}\)</span> is the optimal <span class="math inline">\(\theta\)</span> after fitting the model using maximum likelihood estimation or another optimization method.
Let’s say for a logistic regression model coefficient <span class="math inline">\(\beta_j\)</span>: <span class="math inline">\(\hat{\beta}_j = 1.1\)</span>. <!-- fix vim_ -->
Then an interval could be <span class="math inline">\([0.9; 1.3]\)</span>.
The role of the constant <span class="math inline">\(p\)</span> is similar to the one of the <span class="math inline">\(\alpha\)</span>-level for confidence and credible intervals:
It specifies the size of the interval.
See figure <a href="likelihoodism.html#fig:likelihood-interval">7.2</a>.
Each <span class="math inline">\(\theta\)</span>-value within that interval can be seen as constituting a different hypothesis.
And these hypotheses are compared with the optimal model <span class="math inline">\(\theta = \hat{\theta}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:likelihood-interval"></span>
<img src="figures/likelihood-interval-1.png" alt="1/2 and 1/16 likelihood ratio intervals." width="\textwidth" />
<p class="caption">
FIGURE 7.2: 1/2 and 1/16 likelihood ratio intervals.
</p>
</div>
</div>
<div id="why-frequentism-violates-the-likelihood-principle" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Why Frequentism Violates the Likelihood Principle<a href="likelihoodism.html#why-frequentism-violates-the-likelihood-principle" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many frequentist methods violate the likelihood principle because they require “imagined” experiments.
But frequentists need these theoretic distribution to compute p-values, for example.
Statistical null hypothesis tests also depend on stopping criteria for data collection, which is in conflict with the likelihood principle.</p>
<!-- motivation for example -->
<p>The following example shows how frequentism violates the likelihood principle because of different stopping criteria.
<!-- starting coin example -->
Suppose we have a coin.
We want to find out whether it’s fair, or whether maybe head turns up too often.
I still don’t know why statistician’s are so upset about unfair coins, but that’s the way it is.
We call <span class="math inline">\(\theta\)</span> the probability of head.
We have two hypotheses:</p>
<p><span class="math display">\[H_0: \theta = 0.5 \text{ and } H_1: \theta &gt; 0.5\]</span></p>
<p><span class="math inline">\(H_0\)</span> means that the coin is fair.
<span class="math inline">\(H_1\)</span> claims that head comes up more often than tail.
We define two random variables: the number of heads <span class="math inline">\(X\)</span>, and the number of coin tosses <span class="math inline">\(Y\)</span>.</p>
<!-- two types of experiments -->
<p>We perform two experiments with a different setup but with the same results:</p>
<ol style="list-style-type: decimal">
<li>Toss the coin 12 times. We observe head 9 out of 12 times.</li>
<li>Toss the coin until tail is observed 3 times. The third tail appears on the 12th toss.</li>
</ol>
<p>Both experiments have the exact same outcome, but we defined the stopping criteria for the experiments differently:
In experiment 1), we fixed the number of tosses <span class="math inline">\(Y\)</span>.
In experiment 2), we fixed the number of heads <span class="math inline">\(X\)</span>.
Should we reach different conclusions about the fairness of the coin?
What do you think?</p>
<!-- likelihoodist view of the experiment -->
<p>Both experiments give the same likelihood, up to a constant factor. Experiment 1):</p>
<p><span class="math display">\[L(\theta | X = 3) = \binom{12}{3} \theta^3 (1 - \theta)^9  = 220 \theta^3 (1 - \theta)^9 \]</span></p>
<p>And experiment 2):</p>
<!-- the 2 out of 11 results from the fact that the last toss must be a head -->
<p><span class="math display">\[L(\theta | Y = 12) = \binom{11}{2} \theta^3 (1 - \theta)^9  = 55 \theta^3 (1 - \theta)^9\]</span></p>
<p>So the likelihoodists say that both experiments carry the same evidence.
The likelihood intervals would be the same for both experiments.</p>
<!-- Frequentist view on the experiments -->
<p>Frequentists would come to different conclusions depending on the experiment.
Frequentists include results that have not occurred but depend on how the experiments are conducted.
They assume that <span class="math inline">\(H_0\)</span> is true and infer how the test statistic is distributed in future experiments under <span class="math inline">\(H_0\)</span>.
Then frequentists place the estimated value of <span class="math inline">\(\hat{\theta}\)</span> within this distribution of imagined experiments and see how extreme the result is.
In experiment 1), where the number of tosses is fixed, experiment outcomes of 9, 10, 11, or 12 heads are more extreme than the actual experiment outcome.
In the other experiment, the number of tails is fixed.
More extreme outcomes in experiment 2) are all possible experiments where we observe more than 12 tosses.
This includes, for example, the experiment where the third tail only comes up after 1108 tosses.</p>
<p>When we test <span class="math inline">\(H_0\)</span> vs. <span class="math inline">\(H_1\)</span> in experiment 1), we get:</p>
<p><span class="math display">\[P_{H_0}(X \geq 9) = \sum_{x = 9}^{12} \binom{x}{12} 0.5^x (1 - 0.5)^{12 - x} = 0.073\]</span></p>
<p>At a significance level of <span class="math inline">\(\alpha = 0.05\)</span>, we would not reject the fair coin hypothesis.</p>
<p>For experiment 2), we assume a negative binomial distribution:</p>
<p><span class="math display">\[P_{H_0}(Y \geq 12) = \sum_{y=12}^{\infty}\binom{3 + y - 1}{2} 0.5^y 0.5^3 = 0.0327\]</span></p>
<p>The p-value is now smaller than 0.05, and with that the coin is significantly unfair.</p>
<p>In frequentist inference, the way data are collected and the way experiments are designed affect the results.</p>
<p>This has much more subtle consequences than I’ve illustrated so far.
Imagine a domain expert asks you to perform an analysis with 1000 data points.
As a frequentist, you need to know how those 1000 data points were collected.
What was the stopping criterion for data collection?
If the domain expert only planned to collect 1000 data points, that’s fine.
But if the expert says she would collect more data depending on the outcome of the analysis, then that changes the analysis, which is a violation of the likelihood principle.</p>
</div>
<div id="strengths-3" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Strengths<a href="likelihoodism.html#strengths-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Likelihoodism inherits all the strengths of statistical models.</li>
<li>It’s a coherent modeling approach: all information is contained in the likelihood. Frequentism, in contrast, is more fragmented with long lists of differently motivated statistical tests and confidence intervals.</li>
<li>Like Bayesian inference, likelihoodist inference is also consistent with the likelihood principle. Therefore neither is affected by experimental design, as is the case with frequentism.</li>
<li>Likelihoodism is arguably the most objective of the statistical modeling mindsets. No priors, no imagined experiments.</li>
<li>Likelihoodist ideas can improve the reporting of Bayesian results. For example, Bayesians can additionally report likelihood ratios as evidence.</li>
<li>A significance test might reject <span class="math inline">\(H_0\)</span>, even if the evidence for <span class="math inline">\(H_0\)</span> is greater than for <span class="math inline">\(H_1\)</span>. Likelihoodism doesn’t have this problem.</li>
</ul>
</div>
<div id="limitations-3" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Limitations<a href="likelihoodism.html#limitations-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Likelihoodism doesn’t provide guidance in the form of belief or decision. Evidence is less practical, and the statistician has no certainty about which the final model is and how to work with it. This is the strongest argument against likelihoodism, and maybe the reason why we don’t see it in practice.</li>
<li>To be more specific: There is no theoretical underpinning for saying when there is enough evidence to choose one hypothesis over another.</li>
<li>With a likelihoodist mindset, we can can only compare simple hypotheses where all parameters are specified. Composite hypotheses for ranges of parameters are impossible. Likelihoodism can’t compare <span class="math inline">\(\theta &gt; 0\)</span> versus <span class="math inline">\(\theta \leq 0\)</span>. Only, for example <span class="math inline">\(\theta = 1\)</span> against <span class="math inline">\(\theta = 0\)</span>.</li>
<li>Likelihoodism allows only relative statements. It can’t state the probability that a statistical hypothesis is true – only how its evidence compares to another hypothesis.</li>
</ul>
</div>
<div id="resources" class="section level2 hasAnchor" number="7.7">
<h2><span class="header-section-number">7.7</span> Resources<a href="likelihoodism.html#resources" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>The book “Statistical Evidence: A Likelihood Paradigm” is good introduction to likelihoodism, if you have some background as a statistician.</li>
<li>I’ve found the <a href="http://gandenberger.org/">Greg Gandenberger’s blog</a> super helpful in learning about likelihoodism. He takes a more philosophical viewpoint and argues against likelihoodism and for Bayesianism. This critique is most detailed in his essay “Why I am not a likelihoodist”.<span class="citation"><sup><a href="#ref-gandenberger2016not" role="doc-biblioref">12</a></sup></span></li>
</ul>

</div>
</div>
<h3>References<a href="references-1.html#references-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-richard2017statistical" class="csl-entry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Richard R. Statistical evidence: A likelihood paradigm. Routledge; 2017. </div>
</div>
<div id="ref-hacking1965logic" class="csl-entry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Hacking I. Logic of statistical inference. 1965; </div>
</div>
<div id="ref-gandenberger2016not" class="csl-entry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Gandenberger G. Why i am not a likelihoodist. Ann Arbor, MI: Michigan Publishing, University of Michigan Library; 2016. </div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>The likelihood principle is violated when data is used to inform the prior. For example, empirical priors which make use of the data violate the likelihood principle.<a href="likelihoodism.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="causal-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/modeling-mindsets/edit/master/manuscript/likelihoodism.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
