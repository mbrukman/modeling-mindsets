# Causal Inference {#causal-inference}

* Causal models place random variables into cause-and-effect relationships.
* A model is a good generalization of the world if it encodes causality.
* Causal inference is not a stand-alone modeling mindset, but causal models are either integrated or translated into a [statistical](#statistical-modeling) or [machine learning](#machine-learning) models.

<!-- TODO 

- Visualize Fork, Pipe, Collider, Descendant, see p. 185 in statistical rethinking 
- add personal story at the beginning
- Add some story about causalists to the beginning?
- write about randomized tests / A-B tests, but more as in that  I describe it in another mindset
-->

10-minute course on causality for old-school statisticians, says the note on the door.
You can hear people chanting.
"Correlation does not imply causation. Correlation does not imply causation, ..."

## Causality for the Rescue

<!-- SCQM story -->

Some time ago, I worked with a rheumatologist on an important medical question:
Do TNF-alpha blockers reduce long-term symptoms in patients with axial spondyloarthritis, a chronic condition associated with inflammation of the spine.
In the long-term, the spinal joints may fuse due to new bone formation (ossification). 
TNF-alpha blockers, given regularly as injections or infusions, work wonders to reduce inflammation.
A clinical trial would have been unethical:
Given the proven effectiveness of TNF-alpha blockers in reducing inflammation, it would have been unethical to give a placebo.
The next best option was to use observational data from hospitals and physician's offices.
The registry of patients with rheumatic diseases for which I worked maintained a huge database of Swiss patients with axial spondyloarthritis.
This database contained valuable data about the patients' health histories:
Physician visits, blood work, x-rays, medication history, and so on.
Working with the rheumatologist, I created a statistical model to see if TNF-alpha blockers could prevent ossification.
For these patients, we had spine x-ray images two years apart, which radiologists used to quantify the progression of bone formation.
To predict progression, the model included several variables measured at the time of the first radiograph: patient age, disease duration, inflammation levels, medication, etc.
The result of the analysis was that the drug didn't reduce ossification.
This finding was somewhat consistent with preliminary research findings of others.

By chance, the lead statistician of the patient registry was taking a course on causal inference at about the same time.
She had the epiphany that our model had a flaw.
She drew a diagram visualizing how the drug, inflammation, and the ossification might be related.
Figure \@ref(fig:tnfdag) shows a reduced version of this graph:

```{r tnfdag, fig.cap = "The drug was known to influence (reduce) inflammation. Inflammation was thought to cause ossification (new bone formation). The drug can, in theory, reduce ossification in two ways: directly, or indirectly via reducing inflammation.", fig.height = 3, fig.width = 9}
arr = arrow()
nodes = data.frame(x = c(1, 2, 3),
                   y = c(1, 2, 1),
                   label = c("Drug", "Inflamed\nSpine", "New\nBone"))

edges = data.frame(x =    c(1, 2, 1),
                   xend = c(2, 3, 3),
                   y =    c(1, 2, 1),
                   yend = c(2, 1, 1))
multi = 1.2
radius = 0.2 
lmts = c(0.7, 2.5)
edges$xend2 = edges$xend + c(-radius, - radius, -radius)
edges$yend2 = edges$yend + c(-radius, radius, 0)
edges$x2 = edges$x + c(radius, radius, radius)
edges$y2 = edges$y +  c(radius, -radius, 0)

p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 25, color = "darkgrey") +
  geom_point(size = 23, color = "lightgrey") +
  geom_text(aes(label = label), color = 'black') +
  annotate("text", label = "?", x = 2, y = 1.15, size = 10) +
#  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  theme_void()

add_cc(p)
```


<!-- the solution of the story -->
It became immediately clear where the problem lay in our current model:
Inflammation was a potential mediator of the effect of TNF-alpha blockers on long-term ossification.
Figure \@ref(fig:tnfdag) shows that the effect of the drug can be divided into a direct and an indirect effect.
The total effect is the direct effect of the drug plus any indirect effects, in this case via reduction of inflammation.
We were interested in the total effect, but the way we built the model, the coefficient for the drug could only be interpreted as a direct effect.
The indirect effect was fully reflected in the coefficient for the inflammation level, which was also included in our model.
We therefore removed the inflammation variable.[^adjust-inflammation]
After removal, the coefficient for TNF-alpha blockers could now be interpreted as the total effect of the drug rather than just the direct effect.
Then the model clearly showed that TNF-alpha blockers reduce ossification by reducing inflammation levels.
This sounds like common sense in hindsight, but coming from a frequentist mindset, I was thunderstruck.
That moment was a real revelation and piqued my interest in causal inference.

## Causality

<!-- what is causality -->
<!--$X$ is a cause of $Y$ if changing $X$ changes the probability distribution $P(Y)$. -->
We all have an intuition about causality.
Rain is a possible cause of a wet lawn.
A drug can be a cause of getting well.
An environment policy can be a cause of reduced C02 emissions.
Causality can be expressed as imaginary interventions in random variables:
If you **force** one random variable to take on a certain value, how would the distribution of another random variable (in the real world) change?
A cause is different than an association:
An association is just a statement about an observation.
We know that a wet lawn does not *cause* your neighbor's lawn to be wet.
How do we know that? Try watering your lawn every day for a year, and see if the probability of your neighbor's lawn being wet has changed.
But the wetness of the two lawns is associated:
If you find that your lawn is wet, the probability that your neighbor's lawn is wet is high.
The reason for this association is, of course, rain.
Such common causes are called confounders.

<!-- not for the statisticians -->
The archetypal statistician avoids talking about causality.
At least that's my experience, having done a bachelor's and masters in statistics.
What I learned about causality in those 5 years can be summed up two statements: 1) All confounders must be included in the statistical model as dependent variables, and 2) correlation does not imply causation.
We were taught not to interpret statistical models causally and to view causality as an unattainable goal.
We were taught to ignore the elephant in the room.

<!-- a statisticians mantra goes against science -->
"Correlation does not imply causation"  is really a mantra you hear over and over again  when learning about statistics.
I find this very strange, especially considering that statistical modeling is supposedly THE research tool of our time.
Isn't research all about figuring out how the world works?
The "how" implies, at least for me, that scientists are supposed to uncover causal structures.
The truth is that most results are interpreted causally anyways.
By domain experts, by lay people, and by the media.
Whether the statistician likes it or not.
So shouldn't everyone at least try to fit the model with causality in mind?
Fortunately, some people think we should put causality first.

Welcome to the **causal inference** mindset.

## The Causal Mindset

<!-- causal inference framework in a nutshell -->
The causal inference mindset places causality at the center of modeling.
The goal of causal inference is to identify and quantify the **causal** effect of a random variable on the outcome of interest.

<!-- Relation to other mindsets -->
Causal inference could be seen as an "add-on" to other mindsets such as [frequentist](#frequentism) or [Bayesian](#bayesian) inference, but also for [machine learning](#machine-learning).
But it would be wrong to see causal inference as just a icing on the cake of the other mindsets. 
It's much more than just adding a new type of method to another mindset, like adding support vector machines to supervised learning.
Causal inference challenges the culture of statistical modeling.
It requires the modelers to think more about the data-generating process, to talk explicitly about causes and effects.

<!-- mindsets without causality are broken -->
It's quite surprising how many models are "broken" because they ignore causal reasoning.
A lack of causal reasoning can mean that the analysis of a research paper is invalid, or that a machine learning model in a product is vulnerable to changes in data distribution or adversarial attacks.
Take the Google Flu prediction model as an example.
Google predicted flu outbreaks based on the frequency of Google searches.
The prediction model was clearly not a causal model.
If it were causal, it would mean that you one can could a flu outbreak by searching Google for certain terms.
For example, the flu detection model missed the 2009 non-seasonal flu outbreak. [@lazer2014parable]
The machine learning model's performance quickly degraded as search patterns changed over time.
For the causal modeler, a model that relies only on associations is as short-lived as a fruit fly.
A model generalizes well only if it encodes causal relationships.
A causal flu model might rely on the virulence of current flu strains, the number of people vaccinated, forecasts of how cold the winter will be, etc.

<!-- data can't speak for itself -->
You can study the data as closely as you like, but you can't fully discern just from the numbers what causal structures produced those data.
You can automatically infer associations from the data, but even the simplest causal structures are ambiguous.
Sunshine duration on a given day might be considered causal for the number of park visitors.
In a dataset, both variables would appear as columns with numbers in them.
And if we would calculate the correlation, we would find that sunshine and park visitors are positively correlated.
The causal relationship is clear to everyone:
The sunshine duration influences the park visits.
Park visitors don't control the sunshine, and even the smokiest BBQ won't create enough clouds to change the sunshine duration.
But this causal direction is not obvious to your computer.
No matter which of the variables you target, the computer will comply and fit the model.
*Breaking news: The government has banned visits to the park to cool down the current heat wave.*

Assumptions have to be made to decide on causal directions.
These assumptions are often not be testable and therefore a subjective choice of the modeler.
This creates a target for criticism of the causal inference mindset.
On the other hand, causal inference makes causal assumptions explicit and encourages discussions.
When two causal modelers have different opinions a particular causal direction, they have a way of talking about those differences.

Let's take a look at the best way to make causal structures explicit: The directed acyclic graph.

## Directed Acyclic Graph (DAG)

<!-- short DAG intro -->
Causal inference comes with a tool for visualizing causal relationships: The directed acyclic graphs, DAG for short.
A DAG, like the one in Figure \@ref(fig:dag), makes it easy to understand which variable is a cause of another variable.
Variables are represented as nodes and the causal direction is illustrated by an arrow.
DAGs must be acyclic, meaning arrows are not allowed to go in a circle.
For example, adding an arrow from $Y$ to $X_1$ in Figure \@ref(fig:dag) would make the DAG cyclic, and most causal frameworks can't handle that.  

```{r dag, fig.cap = "A directed acyclic graph (DAG) with 5 variables.", out.width = "\\textwidth", cache = TRUE}
arr = arrow()
nodes = data.frame(x = c(1, 3, 2, 2, 2),
                   y = c(2, 2, 3, 2, 1),
                   label = c("Y", "X1", "X2", "X3", "X4"),
                   role = c("target", "include", "include", "exclude", "exclude"))

edges = data.frame(x =    c(2, 2, 2, 3, 1, 3),
                   xend = c(1, 3, 1, 2, 2, 2),
                   y =    c(3, 3, 2, 2, 2, 2),
                   yend = c(2, 2, 2, 2, 1, 1))
multi = 1.2
radius = 0.2 
lmts = c(0.9, 3.1)
edges$xend2 = edges$xend + c(radius, -radius, multi * radius, multi * radius, -radius, radius)
edges$yend2 = edges$yend + c(radius, radius, 0, 0, radius, radius)
edges$x2 = edges$x - c(radius, -radius, multi * radius, multi * radius, -radius, radius)
edges$y2 = edges$y -  c(radius, radius, 0, 0, radius, radius)

p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 21, color = "darkgrey") +
  geom_point(size = 20, color = "lightgrey") +
  geom_text(aes(label = label), color = 'black') +
  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  theme_void()

add_cc(p)
```

<!-- Structures in a DAG -->
What can we see from the DAG in Figure \@ref(fig:dag)?
Variables $X_2$ and $X_3$ are direct causes of the target $Y$.
$X_1$ affects $Y$ only indirectly through $X_3$.
And $X_4$ is not a cause of $Y$, but $Y$ causes $X_4$ together with $X_1$.

But how do we know where to put arrows and in which direction they should point?
There are several guides here:

* Good old common sense, such as knowing that park visitors can't control the sun.
* Domain expertise.
* Direction of time: We know that the elevator comes because you pressed the button, not the other way around.
* Causal structure learning: To some extent we can learn causal structures automatically. But this usually leads to multiple, ambiguous DAGs.

DAG is a great tool for building causal models, but not all approaches rely on DAGs.

## Many Frameworks For Causality

There are bewildering number of "schools", frameworks, and individual models for causal inference.
And they can differ in notation and approaches. [@hernan2010causal]
I find this lack of uniformity is the biggest barrier to entry into the causal inference mindset.
You can choose different introductory books on Bayesian inference, and the basic notation, language and presented methods will be mostly the same.
But causal inference is just a bit more messy.
So don't despair, it's not you, it's causal inference.
Anyway, here is a brief, non-exhaustive overview of causal modeling approaches to give you an idea of what's out there:

* Much causal inference consists of experimental design rather than causal modeling of observational data, such as clinical trials or A/B tests. Causality claims are derived from randomization and intervention.
* Observational data can resemble an experiment, which some call a "natural experiment". When John Snow studied cholera, he had access to data from a natural experiment. John Snow identified contaminated drinking water as the source of cholera because customers of one water company were much more likely to get cholera than customers of the other.
* Propensity score matching attempts to estimate the effect of an intervention by matching data points taking into account differences in other variables.
* Probably the most general and coherent framework for causal inference comes from statistician Judea Pearl. This "school" includes the do-calculus[@pearl2012calculus], structural causal models, front- and backdoor criteria, and many other tools for causal inference. [@pearl2009causal]
* The potential outcomes framework [@athey2016recursive] is another larger causal "school" used mainly for studying causal effects of binary variables.
* Causal discovery or structure identification is a subfield of causal inference that aims to construct DAGs from observational data.
* Mediation analysis can be used to examine how causal effects are mediated by other variables.
* There are many individual methods that aim to provide causal modeling. One example is "honest causal forests", which are based on random forests and used to model heterogeneity in treatment effects.[@athey2016recursive]
* ...

All approaches have in common that they start from a causal model.
This causal model can be very explicit, for example in the form of a DAG.
But it can also be hidden in  the assumptions of a method.
The final estimate, however, is always a statistical estimator or a machine learning model or something similar.
But how do you get from a causal model to a statistical estimator?


## From Causal Model to Statistical Estimator

<!-- observational data it is -->
In many cases we can't perform experiments because they are infeasible, too expensive or too time-consuming.
But often we have observational data from which we want to infer causal effects.
With observational data, the first casualty is causality -- at least from the point of view of non-causalists.
When causal modelers see observational data, they start stretching and warming up their wrists in anticipation of all the DAG-drawing and modeling to come.

<!-- from causal to observational -->
Causal modelers claim that you can estimate causal effects even for observational data.
I am willing to reveal their secret:
Causal modelers use high-energy particle accelerators to create black holes.
Each black hole contains a parallel universe in which they can study a different what-if scenarios.
Joke aside, there is no magic ingredient for estimating causal effects.
Causal modeling is mainly a recipe for translating causal models into statistical estimators, in the following 4 steps:[@pearl2009causal]


1. Formulate causal estimand.
1. Construct causal model.
1. Identify statistical model.
1. Estimate effect.


<!-- Step 1: Causal estimand-->
Let's now look at the of the steps.
The first step is to formulate the causal estimand.
That means defining the causes and target that we are interested in.
The estimand can be the effect of a treatment on a disease outcome.
It can be the causal influence of supermarket layout on shopping behavior.
Or it can be the extent to which climate change can be attributed to a particular heat wave.

<!-- Step 2: Causal Model -->
Once the causal estimand is formulated, we can derive the causal model.
The causal model can be built in the form of DAG.
And while it doesn't have to be a DAG, I think the DAG is the most meaningful "storage" and visualization of a causal model.
I addition to the target and potential causes, all other variables relevant to both should be included as nodes in the DAG.
Then the modeler must draw the causal relationships as arrows to finish the DAG.

<!-- Step 3: Identify -->
In the identification step, the causal modeler translates the causal model into a statistical estimator.
Not all causal estimands can be estimated with observational data.
In particular,  if a confounder is missing, we can't estimate the causal effect.
Identification can be a complicated, but there are also many simple rules that tell you which variables to include in the statistical model and which to exclude:

* Include all confounders, the common causes of both the variable of interest and the outcome. For example in Figure \@ref(fig:dag-rules) $X_2$ confounds $X_1$ and $Y$.
* Exclude colliders. $X_4$ is a collider for $Y$ and $X_1$. Adding colliders to a model opens an unwanted path.
* Exclude mediators. If we want to measure the causal effect of $X_1$ on $Y$, we have to exclude $X_3$. Including $X_3$ would block the path between $X_1$ and $Y$, and we would incorrectly conclude that $X_1$ has no effect on $Y$.

```{r dag-rules, dependson = "dag", fig.cap = "To understand the causal effect of $X_1$ on $Y$, we have to build a regression model with $Y$ as the target and $X_1$ and $X_2$ as predictor variables."}
library(ggplot2)
p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 22, color = "black") +
  geom_point(size = 20, aes(color = role)) +
  geom_text(aes(label = label), color = 'white') +
  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  scale_color_manual(values = c("grey", "black", "darkgrey"), guide = "none") +
  theme_void()

add_cc(p)
```

<!-- Step 3: Estimation -->
In the end, we have an estimator or model that can be estimated or fitted without any special magic.
Often it's a regular frequentist or Bayesian model, or a supervised machine learning model.
This means that once the model is estimated, we can interpret the model it as we normally would.
But since the model is also the result of causal reasoning, we may interpret the effect of interest in causal terms.

To estimate causal effects of other variables, all steps must  be repeated.
The reason for this is that the identification may lead to a different set of variables for which the model must be adjusted. 

## Strengths

* Causality is central to modeling the world, and causal inference is **the** mindset to embrace that fact.
* I think most modelers actually want causal models. Of course, scientists want causal explanations to better understand the world. Modelers in industry also want causal models so they can understand the impact of  marketing campaigns, for example.
* Only causal models generalize well because they are more robust to changes in the environment. Or put another way: Non-causal models break down more easily, since they are based on associations.
* Causal inference is a rather flexible mindset that extends many other mindsets such as frequentism, Bayesianism, machine learning.
* DAGs make causal assumptions explicit. If you want to take away just one inside from this chapter, or from causal inference in general, it should be DAGs as a way of thinking and communicating.
* You might say that causal modeling with observational data is not possible. The truth is hat once models are out of the hand of the modeler, in many cases they are interpreted causally. Why then not make an effort to introduce some of the best practices from causal inference?

## Limitations

* Many modelers stay away from causal inference for observational data because they say causal models are either not possible or too complicated. 
* Confounders are especially tricky. For a causal interpretation, you have to assume that you have found all the confounders. But one can never prove that one has identified all confounders.
* There are many schools and approaches to causal inference. This can be very confusing for newcomers.
* Causal modeling requires subjective decisions. The causal modeler can never be sure that the causal model is correct.
* Predictive performance and causality can be in conflict: Using non-causal variables may improve predictive performance, but may undermine the causal interpretation of the model.

## Further Reading

* Free book: Causal Inference: What If [@hernan2010causal]

[^adjust-inflammation]: Shouldn't inflammation levels be a confounder because it affects both the decision to treat and the ossification? Or is it a mediator? It depends on the time of the measurement. In the faulty model inflammation was considered after treatment started, making it a mediator of the drug. Later, we  adjusted the model to include inflammation before the start of treatment, making it a confounder.

