# Reinforcement Learning {#reinforcement-learning}

<!-- 
TODO:

- find out examples of reward function: AlphaGo, Starcraft, self-driving cars
- List some reinforcement learning algorithms: Q-learning, MDP?, table-based methods, ... in general, there are many different types of reinforcement learning algorithms (next to deep reinforcement learning): Q-learning, SARSA, ...
- Write about reward shaping somewhere
-->

<!-- topics that were left out

- bellman equations
-

-->

* Reinforcement learning is a framework for modeling interactive decisions of an agent based on rewards.
* Applications in robotics, games, complex systems and simulations.
* Together with [supervised](#supervised) and [unsupervised](#unsupervised-ml), reinforcement learning is a [machine learning](#machine-learning) mindset.


<!-- motivation: joke about how static other mindsets are -->
*Three machine learners attend a conference dinner.
Before them is a lavish self-service buffet, constantly replenished by the service staff.
The dumplings are in high demand, and therefore not available most of the time.
The machine learners decide to increase their chances for dumplings.
The supervised learner starts collecting data on dumpling arrivals.
Surely, the arrival of the delicacy must be predictable!
The unsupervised learner thinks bigger.
In which cluster of dishes does the cluster occurr?
The reinforcement learner merely leaves the table, only to return a few minutes later.
With dumplings. A full plate.
The supervised and unsupervised learners gasp in surprise.
"How did you get all these dumplings?" they want to know.
"First I tried my luck at the buffet, but of course the dumplings were gone.", the reinforcement learner explains.
"Then I thought my options.
The simplest action seemed to be to ask a waiter.
My action was rewarded me with a full plate of dumplings!"
The other two were mind-blown.
How could they have known that proactively interacting with the environment is an option?*



<!-- motivation: super-human gamers -->
Chess, Go, StarCraft II, Minecraft, Atari games, ...
Many people enjoy playing these games, or might even do so professionally.
But they aren't alone.
Computers can play them to.
And they play them at a super-human level.
Many of these games are also very complex.
Go has $10^{127}$ possible board positions, more than there are atoms in the universe (around $10^{78}$ to $10^{82}$).
StarCraft II is a complex real-time strategy game, which requires planning, resource management and military tactics.
These achievements were made possible by machine learning.
But not by supervised or unsupervised learning.
It's the third modeling mindset that outplayed us.
A modeling mindset that gives the computer a "brain" and makes it an agent acting in an environment.

Welcome to **reinforcement learning**!

## The Reinforcement Learning Mindset

<!-- reinforcement learning theory in a nutshell -->
At the core of a reinforcement learning model is the agent.
This agent doesn't sell houses, it's doesn't fight Neo, and it's not investigating crimes.
No. The reinforcement learning agent interacts with an environment with the goal of maximizing rewards.
This environment can be a video game, a city map, a cooling system, an assembly line in a factory, ...
The agent is an entity like a Go player [@silver2016mastering], a route planner, a cooling control unit[@li2019transforming], robotic arm [@gu2017deep], self-driving car steering unit [@kiran2021deep], an image segmentation unit [@wang2018outline].  
The agent observes the environment, but also acts within it and thereby possibly change it.
But how does the agent choose it's actions?
Similar to humans, the agent is "motivated" by rewards:
Beating the other players in StarCraft, getting the temperature right in the building, collecting coins in Super Mario.
The "brain" of the agent is called the policy.
The policy is like a guide that says how to act in which situation.
<!-- A policy can be deterministic (Do A) or stochastic (Do A with 90% probability and B with 10%). -->

```{r rl, fig.cap = "Reinforcement Learning."}
r1 = rectFun(c(0, 0))
r2 = rectFun(c(0, 5)) 

p = ggplot(mapping = aes(x = x, y = y)) +
  geom_path(data = r1) +
  annotate("text", label = "Environment", x = 0, y = 5) + 
  geom_path(data = r2) +
  annotate("text", label = "Agent", x = 0, y = 0) + 
  annotate("curve", x = 1, y = 0, xend = 1, yend = 5, size = 1, curvature = 0.5, arrow = arrow(ends = "first"))  +
  annotate("label", x = 1.7, y = 2.5, label = "Observation, Reward") +
  annotate("curve", x = -1, y = 5, xend = -1, yend = 0, size = 1, curvature = 0.5, arrow = arrow(ends = "first")) +
  annotate("label", x = -1.7, y = 2.5, label = "Policy, Action") +
  scale_x_continuous(limits = c(-2.1, 2.1)) +
  theme_void()

add_cc(p)
```

<!--  RL is dynamic -->
Reinforcement learning is a dynamic modeling approach.
If you fully embrace this modeling mindset, you see tasks as the computer interacting with it's surroundings.
And not, like in other mindsets, working with static snapshots of the world.
The idea of interaction of computer and environment just isn't part of all the other modeling mindsets such as supervised learning or Bayesianism. 
In most modeling mindsets, you usually first collect data and then build the model.
In reinforcement learning the data are produced by the agent interacting with the environment. [^pretrained]
The agent chooses which states to explore, and therefore which data to generate.
The computer is running it's own experiments and learning from them.
The agent cycles through:

- Observation: Look at the world.
- Action: Interact with the world.
- Reward: Get feedback on the action.


<!-- RL is holistic -->
This dynamic or lack thereof has further implications.
Outputs from computer models often affect their environment.
But in other mindsets, this interaction is difficult to model.
Think about pricing a product.
A high price means more revenue per sale, but less customers.
A low price means more customers, but less revenue per sale.
The goal is to find the optimal price.
You could answer the question with supervised learning.
Simply train a model to predict number of sales given the price and potentially other factors.
Training such a model requires access to historic data with varying prices.
But even if you had such data, it's likely to be inadequate.
It's likely that the data was not generated by some experimental design.
It could be that the ideal price is higher or lower than was ever set.
And supervised learning can only learn from what has already happened, but can't explore new things.

<!-- RL is holistic part II -->
Reinforcement learning can handle this dynamic situation.
Changing the price changes the "environment", in our case the sales, inventory levels, and so on.
Here, reinforcement learning is a way to run experiments.
It can deal with the trade-off between exploring new prices and exploiting already learned pricing strategies. 
This makes reinforcement learning a much more holistic approach.
Especially compare to stale prediction models.

<!-- still ml mindset -->
Reinforcement learning is a typical machine learning mindset.
Maybe even more than supervised and unsupervised learning.
A motivation of machine learning is to make the computer act intelligently.
We humans can better relate to a machine if it is intelligent in a human sense (and not just a calculator or Excel sheet).
Reinforcement learning, by making the computer an entity in an environment, exactly transports this idea of intelligent intelligence.
When a computer acts intelligently in a game, maybe in a seemingly human way, we are impressed.

<!-- ml mindset on technical level as well -->
Also on the technical level is reinforcement learning a typical machine learning mindset.
We don't care how exactly the agents policy is implemented -- as long as it works.
And the "working" part can be measured rather directly through the sum of the rewards.


<!-- the core of RL -->
<!--
To decide whether a problem can be solved with reinforcement learning, one has to:

- Define the problem as an agent interacting with an environment
- Define actions that can be taken.
- Define how the environment is encoded: Input pixels? game state?
- Define a reward function. This is very delicate.
- Choose a reinforcement learning algorithm, depending on cardinality of input and action space, difficulty of the problem and so on.
-->

  
###  Reward and Value

<!-- agents are all too human -->
Laying on the couch is easy; working out is difficult.
Why?
There are immediate negative reward associated with working out: 
It's exhausting, you have to shower afterwards, you have to plan, you have to adjust other plans, ...
There are huge positive rewards as well:
Becoming fit and strong, reducing risk for heart attacks and many other health problems, prolonging your life, ... 
These positive rewards are delayed.
Only after weeks of workouts will you reap the first rewards.
Reinforcement learning agents have to deal with the same problem.
Rewards are usually delayed.
In addition, rewards can also be sparse.
Only after an entire game of Go is there a reward, either +1 for a win or -1 for a loss.
If the agent made 100 moves in that game, how should it know which moves where good, and which ones were bad?

<!-- Value function -->
A solution is to assign a value to each state -- even if there is no reward.
If we have just a few possible states, like in Tic-tac-toe, we can tabulate all possible states and write the values in the table.
If states are continuous or the space is too big, we can have a value function.
The value function accepts a state as input, or possibly a combination of state and action.
And the output is the value of the state or state/action.

<!-- what is a value -->
But what exactly IS a value?
In simple terms, the value judges how good it is for the agent to be in this state.
The agent should seek out higher value states.
The value is the expected reward given a state or a state-action pair.
You can think of value as spreading the reward backwards in time, like jam across bread.
When you exercise, it's because you know the value of exercising.
You imagine the future reward of your actions and value the current state accordingly.
Or maybe you don't think of the value all because working out has become a habit.
It has become your policy.

<!-- how to learn value functions? -->
Rewards are provided by the environment, but the values are not provided.
The values or the value function can only be estimated.
There are various ways to learn the value function.
One way is to turn it into a supervised learning task!
<!-- predict the cumulated rewards given a certain state. -->
The Go algorithm Alpha Zero, for example, did exactly that.
By self-play, Alpha Zero collected a dataset of state - reward pairs.
The researchers trained a neural network on this dataset to predict win (+1) or loss (-1) given the game state.
Another approach to learning the value function is Monte Carlo estimation:
We start from random initial states, follow the current policy of the agent and accumulate the rewards.
Then we average the rewards for each state.
Monte Carlo estimation only works for environments with few states.

```{r rl-trajectory, fig.cap = "Trajectory of a reinforcement learning agent through the state space, with a reward at the end.", fig.width = 14, fig.height = 6}

# State size
ssize = 12
# Actor size
asize = ssize
# label.size
lsize = 1.2
arr = arrow(length = unit("0.2", "inch"))
# arrow size
arrsize = 1.6

p = ggplot() +
  # Time step 1 
  annotate("segment", x = 0.9, xend = 0.9, y = 0.8, yend = -0.8, arrow = arr, size = arrsize) +
  annotate("segment", x = 1.1, xend = 1.1, y = -0.8, yend = 0.8, arrow = arr, size = arrsize) +
  annotate("segment", x = 1.6, xend = 2.4, y = 1, yend = 1, arrow = arr, size = arrsize) + 
  annotate("label", x = 1, y = 1, label = "State 0", size = ssize, label.size = lsize) +
  annotate("label", x = 1, y = -1, label = "Actor", size = asize, label.size = lsize) +
  # Time step 2 
  annotate("segment", x = 2.9, xend = 2.9, y = 0.8, yend = -0.8, arrow = arr, size = arrsize) +
  annotate("segment", x = 3.1, xend = 3.1, y = -0.8, yend = 0.8, arrow = arr, size = arrsize) +
  annotate("segment", x = 3.6, xend = 4.4, y = 1, yend = 1, arrow = arr, size = arrsize) + 
  annotate("label", x = 3, y = 1, label = "State 1", size = ssize, label.size = lsize) +
  annotate("label", x = 3, y = -1, label = "Actor", size = asize, label.size = lsize) +
  # Time step 3
  annotate("segment", x = 4.9, xend = 4.9, y = 0.8, yend = -0.8, arrow = arr, size = arrsize) +
  annotate("segment", x = 5.1, xend = 5.1, y = -0.8, yend = 0.8, arrow = arr, size = arrsize) +
  annotate("segment", x = 5.6, xend = 6.4, y = 1, yend = 1, arrow = arr, size = arrsize) + 
  annotate("label", x = 5, y = 1, label = "State 2", size = ssize, label.size = lsize) +
  annotate("label", x = 5, y = -1, label = "Actor", size = asize, label.size = lsize) +
  # Time step 2
  annotate("segment", x = 7, xend = 7, y = 1, yend = 1.4, size = arrsize) +
  annotate("label", x = 7, y = 1, label = "State 3", size = ssize, label.size = lsize) +
  annotate("label", x = 7, y = 1.5, label = "Reward", size = ssize, label.size = lsize) +
  annotate("label", x = 7, y = -1, label = "Actor", size = asize, label.size = lsize) +
  annotate("segment", x = 7, xend = 7, y = 0.8, yend = -0.8, arrow = arr, size = arrsize) +

  coord_fixed() +
  theme_void() +
  scale_y_continuous(limits = c(-1.1, 1.7)) +
  scale_x_continuous(limits = c(0, 8)) 

add_cc(p, 14)

```

## What to Learn

<!-- what should the agent learn? -->
To me this was the most confusing part: Which function(s) do we actually learn in reinforcement learning?
In supervised learning it's very clear.
We learn the function that maps the features to the label.
But it's not obvious what the reinforcement agent should learn.
And in fact there are many different possible approaches:  

- Learn a complete model of the environment. The agent can query such a model to simulate which would be the best action to take at each time step.
- Learn the state value function. If an agent has access to a value function, it can choose actions that maximize the value.
- Or learn the action value function which takes as input not only the state, but state AND action. 
- Learn the policy directly.

These approaches are not mutually exclusive, but can be mixed and matched.
Oh, and in addition we have many different options **how** we learn these things.
And that depends a lot on the dimensionality of the environment and the actions space.
For example, Tic-tac-toe and Go are pretty similar games.
I imagine the objections of all Go players reading this, but hear me out.
Two players face each other off in a fierce, round-based game of strategy!
The battlefield is a rectangular board with gridded positions.
Each player positions marks on the grid.
The winner is determined by the constellations of the marks.

Despite some similarities, the games differ vastly in difficulty for both humans and reinforcement learning.
Tic-tac-toe is often used as an example in reinforcement learning entry classes.
The first super-human Go agent [beat](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol) the Go champion Lee Sedol in 2016, which was a huge media spectacle and required lots of compute resources.
The relevant differences between Tic-tac-toe and Go are the sizes of action space and state space.

<!-- Tic-tac-toeis easy -->
In Tic-tac-toe, there are at most 9 possible actions and in the order of $10^3$ possible action-state pairs.
The agent can learn to play Tic-tac-toe by Q-learning, a model-free reinforcement learning approach to learn the value of an action in a particular state.
This basically enumerates the state-action pairs and iteratively updates the values, while playing more and more games.
<!-- Go is different -->
In Go there are $\sim 10^{170}$ possible states.  
We can't enumerate the states.
The agent was trained to play Go using deep reinforcement learning (read more on that later).

<!-- Learning a policy -->
<!--
There are many different approaches to learn a policy.

* If the space of state and action are small, you can describe the policies with tables.
* REINFORCE, a Monte-Carlo policy gradient method. Parameterized, differentiable policy function that is learned via gradient.
* Actor Critic Algorithm:
-->

This flexibility in what to learn is both a blessing and a curse.
A blessing because it gives us flexibility to adapt the reinforcement learning approach to different tasks.
A curse because it's not always clear how to approach a reinforcement learning task.

## The Other Mindsets 

<!-- ML -->
Reinforcement learning is the third machine learning paradigm, next to supervised and unsupervised learning.
Reinforcement learning differs from the other two because of it's unique formulation of a decision-making agent in an interactive environment.

<!-- RL versus supervised -->
At first glance, rewards seem similar to ground truth labels in supervised learning.
Especially with the value function, we could learn the policy with supervised learning, right?
Not really.
Supervised learning is unsuitable for sequential decision making which requires balancing exploration and exploitation.
Imagine approach a game like Go with a supervised learning mindset.
We would pick the next move as the target to predict, and the game state as the input.
As training data we could use records of human players.
At best, this supervised approach would mimic human players, but it could never explore any new strategies.
There would be no creative freedom and no path to super-human gameplay.
Next to reinforcement learning, supervised learning seems short-sighted and narrow.
Supervised learning only considers parts of the problem without connecting actions.
Reinforcement learning is a much more holistic approach sequentially connecting interactions.

<!--
Reinforcement learning has links to psychology and neuroscience.
It draws from machine learning, but also operations research, control theory, statistics and optimization.
-->



<!-- RL versus unsupervised learning 
Reinforcement learning is also not unsupervised learning.
The reward is a sparse but strong signal for what to learn.
Unsupervised learning does not have such a unique signal.
Not every reinforcement learning problem has such a clear reward, sometimes they have to be designed.
That brings the two mindsets a bit closer.
But still, they are very different for other reasons:
Unsupervised learning lacks all these ideas of interacting with an environment, delayed rewards and so on. 
In a way, reinforcement learning is more similar to supervised learning.
That's because the rewards resembles a ground truth, even if it works differently from supervised learning.
Putting things together, is there maybe something like unsupervised reinforcement learning?
Indeed, there is.
And the idea is that the reward is not extrinsic, but rather intrinsic.
Similar to clustering: Here we decide on a criterion for what an interesting cluster would look like, without knowing whether the resulting grouping is "true".

-->

<!-- RL versus statistical modeling
Reinforcement learning takes a good scoop from statistical modeling.
How we talk about many concepts in reinforcement learning is in statistical terms.
We talk about probablities for actions, Markov decision processes and so on.
The mindset, however, is very different.
And it boils down to reinforcement learning being a machine learning mindset.
How the policy is learned and so on is not as important as getting the job done.
The statistical modeling mindset would be all about modeling variables explictly, relating them to each other, ...
Again, statistics here is the language with which we describe reinforcement learning, but the mindsets are different.
There is an interesting link to [causal inference](#causal-inference):
Due to reinforcement learning time-dependency, and the reward and so on, the actions have to be causal.
Let's say that in StarCraft the winning player often has the most units.
But if an agent would build a lot of worthless units, it would not win the game.
While the mere number of units is correlated with winning, it's not strictly causal.
Having lots of resources and building the right units is causal for winning.
An agent would not learn a policy that proposes non-causal actions, simply because they will not lead to a reward.
However, the agent can learn to rely on non-causal observations, which makes it vulnerable.
-->


The connection between deep learning and reinforcement learning is a bit more special, so let's go deeper here (pun intended).
In short: it's a fantastic mixture of mindsets.
Reinforcement learning struggled with high-dimensional input and large state spaces.
For example Go was too complex for many reinforcement learning algorithms.
Also other environments where the states are images or videos are challenging to model.
Except when you can use deep learning.


Deep reinforcement learning got many more people excited about reinforcement learning, and about AI in general.
For this fusion of approaches, reinforcement learning is made "deep" by replacing some functions with deep neural networks.
For example the value function or the policy function.
Using deep neural networks allows more complex input, especially images.
A successful example of deep reinforcement learning is Alpha Zero.
Alpha Zero is a reinforcement learning algorithm that can play Go on a super-human level.
Alpha Zero relies on two deep neural networks: a value network and a policy network.
From self-play, a data set is created that stores board state + win/loss.
The value network is trained on this self-play data to predict the value (between -1 and +1) of a Go board position.
The policy network outputs action probabilities based on the Go board (the state).
But the agent doesn't automatically follow the most likely action.
Instead, the policy network works in tandem with a Monte Carlo tree search algorithm.
The Monte Carlo tree search connects the policy with the value of the board, and simulates consequent moves.
The training of the policy network is also intertwined with the Monte Carlo tree search.

<!-- RL in general -->
Deep reinforcement learning is an exciting combination of the two mindsets.
It resulted in great success in many games and is being applied in other fields such as robotics.
But it's rather data intensive, and often [not the right approach](https://www.alexirpan.com/2018/02/14/rl-hard.html).

<!--
## Impressive to Look At

With all the hype, it's quite surprising how little real world applications there are in practice.
The reason is that reinforcement learning is difficult to get right.
Training can be quite unstable.
And the most difficult is, that, in order to get in enough training, it's almost impossible to train it in the real world.
This means that it's either restricted to simulations or the application itself is completely digital, like a game.
And these are the prime starting points: games and simulations.
That's also why the most impressive headlines were games beating human players.
But if you want to train a robotic arm to grap an item and put it into another spot.
It's harder.
You first need a simulation and train the reinforcement learning agent in a simulation.
But it's also difficult to make the transfer from simulation to reality.
From the simulated robotic arm to the physical one.
-->


## Strengths

- Reinforcement learning allows to model the world in a dynamic fashion. And many tasks natural are agent-in-environment setups.
- It's the best approach for planning, playing games, controlling robots and larger systems.
- Actions of the agent **change** the environment. In other mindsets, the model is a mere "observer", which often is a false simplification.
- More than other mindsets, reinforcement learning resembles how humans and animals learn (still quite different though). 
- Reinforcement learning seems to be the closest modeling paradigm for mimicking animal intelligence.


## Limitations

- Reinforcement learning requires that the task involves some form of "agent".
- Very often, reinforcement learning, especially deep reinforcement learning, is just the [wrong approach to a problem](https://www.alexirpan.com/2018/02/14/rl-hard.html).
- Reinforcement learning models can be very difficult to train and reproduce:
  - Learning requires many episodes, because it's sample inefficient.
  - Designing the right reward function can be tricky.
  - The training can be unstable, and might get stuck in local optima.
- Reinforcement learning models are trained in artificial environments. It's difficult to transfer the models into the physical world.
- Model-free or model-based? Learn the policy? Or the value function? Or the action-value function? There are many choice which can be overwhelming.



## References

- Reinforcement Learning, An introduction [@sutton2018reinforcement]
- One of my favorite papers about algorithm "fails" [@lehman2020surprising]. It's focused evolutionary algorithms, not reinforcement learning. But it holds lessons for how tricky it can be to set a good goal to optimize for. Prepare yourself for a program repair robotor that makes software tests easier instead of repairing the software. 



[^pretrained]: Data may be collected beforehand. For example, the Alpha Go algorithm was pre-trained by Go moves from human players (in a supervised learning fashion). [@chen2018recurrent]



