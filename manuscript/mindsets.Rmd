# Mindsets {#mindsets}

<!-- CONTENT TO ADD

* Mindsets are not coherent philosophies
* There are Bayesian versions of p-values and hypothesis tests, but I would consider these approaches a core of the Bayesian mindset. It's subjective.
-->

<!-- from models to mindsets -->
A model is only a bunch of variables, relations, and parameters.
A model alone can't tell us how to interpret the world.
The use and interpretation of the model depends on the mindset from which the model arose.
In order to derive knowledge about the world from the model, we need to make further assumptions.
Consider a linear regression model that predicts regional rice yield as a function of rainfall, temperature, and fertilizer use.
It's a model, not a mindset.
How may we interpret the model?
Can we interpret the effect of fertilizer as causal to rice yield? Or is it just a correlation? <!-- causal inference -->
Would we trust the model to make accurate predictions for the future as well? <!-- supervised ML mindset -->
Can we say anything about the statistical significance of the effect of the fertilizer? <!-- frequentism -->
Or have we just updated prior information about the effect of fertilizer with new data? <!-- Bayesianism -->

Welcome to **Modeling Mindsets**.

A modeling mindset is a specification of how to model the world using data.
Modeling means investigating a real world phenomenon indirectly using a model. [@weisberg2007modeler]
<!-- lenses analogy-->
Modeling mindsets are like different lenses.
All lenses show us the world, but with a different focus.
Some lenses magnify things that are close, some that are far away.
Some glasses are tinted so you can see in bright environments.
When you look through a lens, you see the world in a certain way.
<!-- mindsets -->
With different modeling mindsets, you can look at the modeling task, but the model will focus on different things.
Bayesianism, frequentism, supervised machine learning, generative models, ... these are all different mindsets when it comes to building models from data.
<!-- more detailed differences of mindsets -->
Mindsets differ in how they interpret probabilities -- or whether probabilities are part of the language at all.
While mindsets cover many different modeling tasks, they have some tasks where they really shine.
Each mindset invites you to ask different questions, and so shapes the way you view the world through your model.
In supervised machine learning, for example, everything becomes a prediction or classification problem, while in Bayesian inference, the goal is to update our beliefs about the world using probability theory.


<!-- small and large world -->
Within a  mindset there are two worlds: the model world and the real world (Figure \@ref(fig:mindsets).
Or, as McElreath called them in his book "Statistical Rethinking": the "small" and the "large" world. [@mcelreath2020statistical]
All modeling results are first and foremost statements about the model world, and to be interpreted within the simplified model world..
How and if model results may be transferred from the model world to the real world, depends on the mindset. 

```{r mindsets, fig.cap = "Only when a model is embedded in a mindset can we put it into context with the world."}
library(ggplot2)
library(patchwork)

circleFun <- function(center = c(0,0),diameter = 1, npoints = 100){
    r = diameter / 2
    tt <- seq(0,2*pi,length.out = npoints)
    xx <- center[1] + r * cos(tt)
    yy <- center[2] + r * sin(tt)
    return(data.frame(x = xx, y = yy))
}

# by how much the world circle is shifted
shift = 6

# Data for drawing model circle
dat = circleFun(c(2,2.3), 3, 100)
dat$circle = "c1"
# Data for drawing world circle
dat2 = circleFun(c(2 + shift, 2.3), 3, 100)
dat2$circle = "c2"
dat = rbind(dat, dat2)

p = ggplot(dat, aes(x = x, y = y)) +
 annotate("rect", xmin = 0, xmax = 10, ymin = 0, ymax = 5, color = "black", fill = "white") +
 annotate("label", x = 2 +  shift / 2, y = 5, label = "Mindset", size = 12) + 
 geom_path(size = 2, aes(group = circle)) +
 annotate(x = 2, y = 3.7, geom = "label", label = "Model", size = 9, label.size = 1) +
 annotate(x = 8, y = 3.7, geom = "label", label = "World", size = 9, label.size = 1) +
 annotate("text", x = 8, y = 2.3, label = "?", size = 18) +
 annotate("segment", x = 4, xend = 6, y = 2.3, yend = 2.3, arrow = arrow(ends = "both"), size = 2) +
 theme_void() 

nodes = data.frame(x = c(1, 3, 2),
                   y = c(2, 2, 3))

edges = data.frame(x =    c(2, 2, 3),
                   xend = c(1, 3, 1),
                   y =    c(3, 3, 2),
                   yend = c(2, 2, 2))
multi = 1.5
radius = 0.22 
edges$xend2 = edges$xend + c(radius, -radius, multi * radius)
edges$yend2 = edges$yend + c(radius, radius, 0)
edges$x2 = edges$x - c(radius, -radius, multi * radius)
edges$y2 = edges$y -  c(radius, radius, 0)

pmodel = p +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, color = "black", size = 0.5) +
  geom_point(size = 10, color = "darkgrey", data = nodes) +
  annotate("text", label = "alpha", x = 1.4, y = 2.65, parse = TRUE,  size = 5) +
  annotate("text", label = "beta",  x = 2.6, y = 2.65, parse = TRUE, size = 5) +
  annotate("text", label = "gamma", x = 2, y = 1.85, parse = TRUE, size = 5) +
  scale_y_continuous(limits = c(0, 5.3)) +
  theme_void() +
  coord_fixed()

pmodel

```

<!-- a normative frame -->
**Modeling mindsets** are normative:
A modeling mindset distinguishes between good and bad models.
Even though model evaluations are partly based on objective criteria, the choice of a criterion is subjective.
Each mindset has their own set of accepted models and evaluation procedures.

<!-- example: frequentist statistician -->
For a frequentist statistician, a good model of the world is probabilistic and has a high goodness-of-fit to the data.
The residual errors of the model also pass diagnostic checks.
The frequentist rejects the use of prior probabilities as they appear to be subjective.
The frequentist would also not switch to a random forest just because it has a lower mean squared error on test data.
And why would the statistician switch?
From their point of view, the random forest is a poor model of the world.
We learn nothing about the probability distribution of our variables.
We can't do frequentist hypothesis testing of the effects of variables.
The performance of the frequentist's model on unseen test data is not as important to the frequentist.

<!-- permissible questions -->
A modeling mindset limits the questions that can be asked.
Consequently, some questions or tasks are out of scope of the mindset.
Often the questions are out of scope because they just don't make sense in a particular modeling mindset.
Supervised machine learners formulate tasks as prediction or classification problems.
Questions about probability distributions are out of reach since the mindset is: choose the model that has the lowest generalization error given new data.
So the best model could be any function, such as the average prediction of a random forest, a neural network, and a linear regression model.
If the best model can be any function, questions that a statistician would normally ask (hypothesis testing, parameter estimation, ...) become irrelevant to the machine learner.
In machine learning, the best models are usually not classical statistical models.
If the machine learner started asking questions a statistician would ask, they would have to choose a suboptimal model, which is a violation of the mindset.


<!-- cultural -->
**Modeling mindsets are cultural**.
Modeling mindsets are not just theories; they shape and are shaped by communities of people who model the world based on the mindset.
In many scientific communities, the frequentist mindset is very common.
I once consulted a medical student for his doctoral thesis.
I helped him visualize some data.
A few days later he came back, "I need p-values with this visualization."
His advisor told him that any data visualization needed p-values.
His advisor's advice was a bit extreme, and not advice that a real statistician would have given.
However, it serves as a good example of how a community perpetuates a certain mindset.
Likewise, if you were trying to publish a machine learning model in a journal that publishes mostly Bayesian analysis, I would wish you good luck.
And I'd bet 100 bucks that the paper would be rejected.

<!-- culturally accepted -->
The people within a shared mindset also accept the assumptions of that mindset.
And these assumptions are usually not challenged, but mutually agreed upon
At least implicitly.
If you work in a team that has been using Bayesian statistics for some time, you won't be questioning each model anew about whether using priors is good or whether the Bayesian interpretation of probability is legit.
In machine learning competitions, the model with the lowest prediction error on new data wins.
You will have a hard time arguing that your model should have won because it's the only causal model.
If you believe that causality is important, you would simply not participate.
You can only thrive in machine learning competitions if you have accepted the supervised machine learning mindset.

<!-- archetypes -->
The modeling mindsets as I present them in this book are archetypes: pure forms of these mindsets.
In reality, the boundaries between mindsets are much more fluid.
These archetypes of mindsets intermingle within individuals, communities and approaches:
A data scientist who primarily builds machine learning models might also use some simple regression models with hypothesis tests -- without cross-validating the models' generalization error.
A research community could accept analyses that use both frequentist and Bayesian statistics.
A machine learning competition could include a human jury who would award additional points if the model is interpretable and includes causal reasoning.

<!-- sometimes 100% archetypes -->
Have you ever met anyone who is really into supervised machine learning?
The first question they ask is "Where is the labeled data?".
The supervised machine learner turns every problem into a prediction or classification problem.
Or perhaps you've worked with a statistician who always wants to run hypothesis tests and regression models?
Or you had intense discussion about probability with a hardcore Bayesian?
Some people really are walking archetypes. 100\% of one archetype.
But I would say that most people learned one or two mindsets when they start out.
And later they got glimpses of other mindsets here and there.
Most people's mindset is already a mixture of multiple modeling mindsets.
And that's a good thing.
Having an open mind about modeling ultimately makes you a better modeler.

<!-- not clear cut: mindset subsets, mindset overlaps 
This book does not claim to have a sharp scientific taxonomy or ontology of mindsets.
Instead, I was quite liberal in deciding when something could be considered as a mindset.
This means that mindsets can overlap, they can be an extension or subsets of other mindsets.
As the main criterion for pronouncing something a distinct mindset, is when I thought that it's a useful view of the world, and you enhance your knowledge by seeing a particular thing as a modeling mindset.
-->

<!-- differentiators between mindsets 
I had a bit more objective criteria.

* A mindset should have it's own Wikipedia entry.
* A modeling mindset should be "holistic" in the sense that it encompasses a broad range of modeling tasks. Or at least expicitly exclude some tasks out of philosophical reasons. 
* Another, softer, criterion was that there are communities around modeling mindsets. For example, I dedicated a chapter to supervised machine learning, in addition to a chapter on machine learning. Supervised machine learning has such a big community, think of machine learning competitions a la Kaggle.
* A mindset should have at least one unique aspect that distinguishes it from other mindsets (except for the 'meta'-mindset chapters on [likelihood](#likelihood) and [risk-minimization](#risk-minimization).  
-->


<!--

Other mindsets

* Sampling theory or survey sampling with design-based assumptions for modeling. Alternative name: design-based inference. Survey sound people oriented, and it is. But can also mean other samples. Normally, for statistical modeling, it's assumed that the model is unknown and the goal is to find the right model / distributions to represent the data-generating process. with survey sampling we start with the assumption that we know the model, but we have to collect the data with random sampling. It's a data sampling first approach. Very complementary of course. But it's own mindset bc. of the special sampling focus, and stating that sampling is the most important.   https://en.wikipedia.org/wiki/Statistical_assumption#Classes_of_assumptions and https://en.wikipedia.org/wiki/Survey_sampling
  * https://ies.ed.gov/ncee/pubs/20174025/pdf/20174025.pdf
* Generative models - machine learning
* Discriminative models - machine learning
* Symbolic AI
* Logic Programming
* ML + IML
* Non-parametric statistics: Own mindset bc we don't assume the distributions any longer, which are elementary to frequentists and bayesians. Instead we do bootstrap, permutation tests and so on. computationally expensive, but fewer explicit choices for distributions.
* Experiment of design / randomized trials: Mindset is that you don't work with observational data. Your modeling starts with data collection process. Modeling after not as important as collecting data. even comparison of means possible afterwards.
* Interpretable machine learning and sensitivity analysis
* Generative modeling: GANs, Naive Bayes, anything that produces samples. Modeling mindset: We want to replicate the data-generating process. Applications in art also. But can also be used for discriminative tasks. Models overlap with other approaches.
* Deep Learning: Own mindset because of some unique attributes that allow to think differently about modeling: embedding, transfer learning, generative modeling (link to the mindset at least), feature learning,  
* Unsupervised learning / clustering / association rule learning / outlier detection: Mindset of finding general patterns inthe data. Not driven by a distinct $Y$, but instead by average big trends. 
* Propensity probabilities? https://en.wikipedia.org/wiki/Propensity_probability not complete modeling mindset, but at least a different interpretation of probability
* Data visualization: Just plot stuff. Humans are good at recognizing stuff. Also works for data that is not representative. No assumptions. You see much more than when only, e.g. hypothesis testing or predictive performance. You can see for example when errors happen.
* Operations research: https://en.wikipedia.org/wiki/Operations_research . 
 is focused on making decisions. sometimes interest in extreme value. tied to organization and systems.
* Representation learning? https://en.wikipedia.org/wiki/Feature_learning
* Semi-parametric models? Or at least mention it in liklelihood mindset. For example the cox model.
* Decision theory: As alternative to frequentism, Bayesianism, likelihoodism. Focus is on decisions even more so than on frequentism. Strongly builds on statistical mdeling mindsets. Contra argument: not very unified field https://people.kth.se/~soh/decisiontheory.pdf   https://en.wikipedia.org/wiki/Decision_theory

TODO: Write about inductive vs deductive inference

-->



