# Design-based inference {#design-based-inference}

<!-- TODO: Split into design-based (meta framework), survey sampling, design of experiments -->
<!-- TODO: Malte zum reviewn schicken -->

<!-- TODO: Read in detail https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2856970/ -->

<!-- TODO: Checkout Stichprobentheorie Material from LMU -->
<!-- TODO: Checkout Experiment of Design material from LMU -->
<!-- TODO: Add reference to this chapter from the causal inference chapter -->

Design-based modeling favors strong research designs over complex statistical models. 
Instead of focus on model, focus on sampling data.
Sample right data, have simple estimators for your quantities of interest.
Design-based vs. model-based.

Design-based: Researcher designs experiment which generates data. 
I see two things as design-based:
* just sampling data in a designed, randomized way
* also do experiment

Why does such a design-based approach reduce the need for complex modeling?

e.g. want to know the effect of a nudge on eating behaviour of people.
For example, do people buy more things if they cost 0.99 instead of 1?
It's a simple question, at first.
You have data from supermarkets.
These set their prices themselves, so some have 0.99 prices, some have 1s.
But they differ in many other things.
Some are expensive, some are small, some have more customers, they have different products, ...
So we have to adjust for all these things, as these might be confounders of relationship between price and amount sold.

Version 2: You design an experiment.
You randomize the products, the supermarkets, the days on which price is done and so on.
Then you collect data.
If done well, you can simply take the mean of number of products with 0.99 and subtract them from the 1 product amount sold.

BUT: Still often both are combined: design-based inference and a model afterwards.
Remember, this chapter is about the archetype of design-based inference.
In reality, even when there is a good research design, researchers and data scientists still use more or less complex models.
For example in social sciences (see http://thaddunning.com/wp-content/uploads/2010/05/Dunning_Rethinking-Social-Inquiry.pdf).

Extreme view: We can ONLY make inference when we have completely random samples.
That's an archetype of design-based view.


In model-based mindsets, like frequentism, we control for variables via the regression models.
We want to remove the effect of age on the income for example, as we are interested only in the profession.
So we have to include it into the model as a variable (predictor / covariate).
For design-based modeling, we design our sampling / experiment to account for such things.
For example by stratified sampling design in all age and and professions.

## Google Cookie Experiment

* Design that is sequential
* Closest is maybe experimental design where the environment is manipulated to collect data
  * Google Cookies were created by experimental design.
  * The design choices were proposed by Bayesian optimization
  * https://blog.google/technology/research/makings-smart-cookie/



## Assumptions

* Variance comes from sampling alone
* With good randomization, we can remove confounding
* random sampling is the basis for inference
* We don't have an infinite population, as we assume for model-based inference, like frequentism
* Design-based inference assumes a finite population.
* For example: Effect of treatment on disease outcome. Population: All patients with this disease. Goal: get good sample of these patients, do experiment, and that's that. 
* Outcome is still random variable, but not due to epistemic randomness, but rather because of the sampling procedure
* Finite population = all data points that have a chance of getting into the sample
* Randomness of sampling is required 

## Methods

* Survey sampling
* Randomized clinical trials (doesn't assumes finite population, so only in aspects design-based).



## Neymann Causal Inference Model

https://ies.ed.gov/ncee/pubs/20090061/chapter_2.asp




## Compare to Other Mindsets



Example: Predict outcome of an election (not sure if good example)

* Statistics: Build model based on available data, maybe outcome of last election and so on
* Machine Learning: Use various data, all thrown together. 
* Design-based: Ask people. Do clever design. Call them about who they want to vote. 

Design-based is the closest you can get to causal inference without specifically requiring a complete causal model.
The causal is by randomization.
So we can eliminate confounders.


Hybrid-framework between frequentist model-based frameworks and design-based inference.


What would a design-based modeler say to a image classification system with neural networks?
This type of prediction is outside of design-based inference.
It would say: What is the population of images?
Maybe it's all images on the internet.
It's finite.
We can compute the share of all images that have cats on them.
We just need a good sampling mechanism.
The model does not rely on such a good sampling.
I don't trust it therefore.
Based on it's assumptions, the design-based mindset would reject the idea of machine leanring and prediction.
Design-based requires a finit population.
Machine learning says it works for new data, as long it follows the same distribution.
Let's say a new cat is born.
It's not part of the finite population with which the machine learning model was trained.
Hence this sweet cat baby is not inside the population that we are interested in.
From a design-based inference perspective, the new cat does not interest us.
Heartless, but true.




Design-based also works with bayesian approach.
https://biostats.bepress.com/cgi/viewcontent.cgi?article=1004&context=umichbiostat
Requires to specify a prior distribution for the population outcome $Y$.



<!-- how design-based mindset can enhance other mindsets -->
But the finite population, it's a powerful idea.
Let's extend this idea to the [supervised learning](#supervised) mindset.
You want to build a classificaiton model for images.
What is the entire population that you are looking at?
Where do the images come from?
How would you sample them?
Which images would have a chance of zero to end up in the training data?
By putting on this mindset, you can immediatly find flaws.
You have an angle of thinking about the data.


## Natural Experiments

Sometimes we just have observational data.
But it may be have like experiments.
So we only need to sample from them.

Natural experiments are "as-if" experiments.
As if we did an experiment.
As if the variables we are interested in (such as treatment) were randomly assigned.


A bit exaggerated example:
Company produces drugs.
One production site accidentially produces placebo.
So for a while, some patients receive placebo.
Assuming we can identify placebo patients, we have a natural experiment.



<!-- designed experiments -->
So the critique of $P(Y|X,T)$ is that this is a mere association between $X$ and $Y$.
Let's challenge this by designing an experiment where we have control over the variable that interests us!
This is what happens in clinical trials, which is a randomized and controlled study.
We have control over whom we give treatment and who a placebo.
The randomization ensures that the other variables are, more or less, balanced.
For example, age and so on should follow the same distribution.
Thanks to this variable balance, we can equate the mere assocation $P(Y|X)$ with causation $P(Y|do(X))$.
To make sure that we have as little confounding as possible, we randomize who gets treatment and placebo.
In this case, $P(Y|X, do(T))$ and $P(Y|X,T)$ are the same.

<!-- observational data -->
But we are far away from these designed experiments being applicable everywhere.
Randomized controlled trials are expensive.
Sometimes it's not even ethical to do a clinical trial.
Does smoking kill?
"You, you and you! You are in the smoker group. One pack per day, for the next ten years. Thanks for signing up."
Sometimes it's not possible.
What's the effect of a gene on a disease?
You can't just go and change the gene in a person.
Not yet at least, and even then we might jump into the "unethical" category.
You can't just go ahead and change the GDP of a country.


## Strengths

* Can cancel out lots of confounders.Randomization is one way to remove confounding.
* Simpler models
* No distributional assumptions are needed. All variance and such can be directly derived from the sampling approach.
* Less opportunities for p-hacking and false modeling. For example not always clear which factors to control for. Can be tempting to choose the one the deliver smaller p-value for your hypothesis.
* More realistic assumptions than statistical models.

## Limitations

* Experiments expensive, take long time
* Less useful for predictive modeling
* Only descriptive inference allowed: Because we can only describe the finite population. And nothing outside of it.
* Only simple things can be estimated, such as means.
* The sampling weights can carry implicit assumptions. Not so model-free after all.
* And often, in reality, you need statistical models after all
* For causal questions: You still need some form of causal model. Because this influences your sampling design.
* "n practice, large, complex regression models are often fitted to the data  produced by these strong research designs" http://www.thaddunning.com/wp-content/uploads/2010/05/Dunning_Rethinking-Social-Inquiry.pdf 




References

* https://ies.ed.gov/ncee/pubs/20174025/pdf/20174025.pdf
* http://thaddunning.com/wp-content/uploads/2010/05/Dunning_Rethinking-Social-Inquiry.pdf
* Paper comparing model and design-based inference: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2856970/
* Book on natural experiments, and the design-based approach: https://jonnyphillips.github.io/FLS6415/Class_5/Dunning%20Natural%20Experiments.pdf
