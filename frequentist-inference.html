<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Frequentist Inference | Modeling Mindsets</title>
  <meta name="description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Frequentist Inference | Modeling Mindsets" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Frequentist Inference | Modeling Mindsets" />
  
  <meta name="twitter:description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2022-07-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-modeling.html"/>
<link rel="next" href="bayesian-inference.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>

<style>

#cta-button-desktop:hover, #cta-button-device:hover {
  background-color:   #ffc266; 
  border-color:   #ffc266; 
  box-shadow: none;
}
#cta-button-desktop, #cta-button-device{
  color: white;
  background-color:  #ffa31a;
  text-shadow:1px 1px 0 #444;
  text-decoration: none;
  border: 2px solid  #ffa31a;
  border-radius: 10px;
  position: fixed;
  padding: 5px 10px;
  z-index: 10;
  }

#cta-button-device {
  box-shadow: 0px 10px 10px -5px rgba(194,180,190,1);
  display:none;
  right: 20px;
  bottom: 20px;
  font-size: 20px;
 }

#cta-button-desktop {
  box-shadow: 0px 20px 20px -10px rgba(194,180,190,1);
  display:display;
  padding: 8px 16px;
  right: 40px;
  bottom: 40px;
  font-size: 25px;
}

@media (max-width : 450px) {
  #cta-button-device {display:block;}
  #cta-button-desktop {display:none;}
}


</style>





<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Mindsets</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="what-this-book-is-about.html"><a href="what-this-book-is-about.html"><i class="fa fa-check"></i><b>1</b> What This Book is About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="what-this-book-is-about.html"><a href="what-this-book-is-about.html#who-this-book-is-for"><i class="fa fa-check"></i><b>1.1</b> Who This Book is For</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Models</a></li>
<li class="chapter" data-level="3" data-path="mindsets.html"><a href="mindsets.html"><i class="fa fa-check"></i><b>3</b> Mindsets</a></li>
<li class="chapter" data-level="4" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>4</b> Statistical Modeling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#random-variables"><i class="fa fa-check"></i><b>4.1</b> Random Variables</a></li>
<li class="chapter" data-level="4.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#probability-distributions"><i class="fa fa-check"></i><b>4.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="4.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#assuming-a-distribution"><i class="fa fa-check"></i><b>4.3</b> Assuming a Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-model"><i class="fa fa-check"></i><b>4.4</b> Statistical Model</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i>Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-hypothesis"><i class="fa fa-check"></i>Statistical Hypothesis</a></li>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#interpreting-parameters"><i class="fa fa-check"></i>Interpreting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="statistical-modeling.html"><a href="statistical-modeling.html#joint-or-conditional-distribution"><i class="fa fa-check"></i><b>4.5</b> Joint or Conditional Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="statistical-modeling.html"><a href="statistical-modeling.html#regression-models"><i class="fa fa-check"></i><b>4.6</b> Regression Models</a></li>
<li class="chapter" data-level="4.7" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-evaluation"><i class="fa fa-check"></i><b>4.7</b> Model Evaluation</a></li>
<li class="chapter" data-level="4.8" data-path="statistical-modeling.html"><a href="statistical-modeling.html#data-generating-process-dgp"><i class="fa fa-check"></i><b>4.8</b> Data-Generating Process (DGP)</a></li>
<li class="chapter" data-level="4.9" data-path="statistical-modeling.html"><a href="statistical-modeling.html#drawing-conclusions-about-the-world"><i class="fa fa-check"></i><b>4.9</b> Drawing Conclusions About the World</a></li>
<li class="chapter" data-level="4.10" data-path="statistical-modeling.html"><a href="statistical-modeling.html#strengths"><i class="fa fa-check"></i><b>4.10</b> Strengths</a></li>
<li class="chapter" data-level="4.11" data-path="statistical-modeling.html"><a href="statistical-modeling.html#limitations"><i class="fa fa-check"></i><b>4.11</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="frequentist-inference.html"><a href="frequentist-inference.html"><i class="fa fa-check"></i><b>5</b> Frequentist Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="frequentist-inference.html"><a href="frequentist-inference.html#frequentist-probability"><i class="fa fa-check"></i><b>5.1</b> Frequentist probability</a></li>
<li class="chapter" data-level="5.2" data-path="frequentist-inference.html"><a href="frequentist-inference.html#estimators-are-random-variables"><i class="fa fa-check"></i><b>5.2</b> Estimators are Random Variables</a></li>
<li class="chapter" data-level="5.3" data-path="frequentist-inference.html"><a href="frequentist-inference.html#null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>5.3</b> Null Hypothesis Significance Testing</a></li>
<li class="chapter" data-level="5.4" data-path="frequentist-inference.html"><a href="frequentist-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.4</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.5" data-path="frequentist-inference.html"><a href="frequentist-inference.html#strengths-1"><i class="fa fa-check"></i><b>5.5</b> Strengths</a></li>
<li class="chapter" data-level="5.6" data-path="frequentist-inference.html"><a href="frequentist-inference.html#limitations-1"><i class="fa fa-check"></i><b>5.6</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>6</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayes-theorem"><i class="fa fa-check"></i><b>6.1</b> Bayes Theorem</a></li>
<li class="chapter" data-level="6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prior-probability"><i class="fa fa-check"></i><b>6.2</b> Prior Probability</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#picking-a-prior"><i class="fa fa-check"></i>Picking a Prior</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#likelihood"><i class="fa fa-check"></i><b>6.3</b> Likelihood</a></li>
<li class="chapter" data-level="6.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#evidence"><i class="fa fa-check"></i><b>6.4</b> Evidence</a></li>
<li class="chapter" data-level="6.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-probability-estimation"><i class="fa fa-check"></i><b>6.5</b> Posterior Probability Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#sample-from-the-posterior-with-mcmc"><i class="fa fa-check"></i>Sample From the Posterior with MCMC</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summarizing-the-posterior-samples"><i class="fa fa-check"></i><b>6.6</b> Summarizing the Posterior Samples</a></li>
<li class="chapter" data-level="6.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#from-model-to-world"><i class="fa fa-check"></i><b>6.7</b> From Model to World</a></li>
<li class="chapter" data-level="6.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#simulate-to-predict"><i class="fa fa-check"></i><b>6.8</b> Simulate to Predict</a></li>
<li class="chapter" data-level="6.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#strengths-2"><i class="fa fa-check"></i><b>6.9</b> Strengths</a></li>
<li class="chapter" data-level="6.10" data-path="bayesian-inference.html"><a href="bayesian-inference.html#limitations-2"><i class="fa fa-check"></i><b>6.10</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="likelihoodism.html"><a href="likelihoodism.html"><i class="fa fa-check"></i><b>7</b> Likelihoodism</a>
<ul>
<li class="chapter" data-level="7.1" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-principle"><i class="fa fa-check"></i><b>7.1</b> Likelihood Principle</a></li>
<li class="chapter" data-level="7.2" data-path="likelihoodism.html"><a href="likelihoodism.html#law-of-likelihood"><i class="fa fa-check"></i><b>7.2</b> Law of Likelihood</a></li>
<li class="chapter" data-level="7.3" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-intervals"><i class="fa fa-check"></i><b>7.3</b> Likelihood Intervals</a></li>
<li class="chapter" data-level="7.4" data-path="likelihoodism.html"><a href="likelihoodism.html#why-frequentism-violates-the-likelihood-principle"><i class="fa fa-check"></i><b>7.4</b> Why Frequentism Violates the Likelihood Principle</a></li>
<li class="chapter" data-level="7.5" data-path="likelihoodism.html"><a href="likelihoodism.html#strengths-3"><i class="fa fa-check"></i><b>7.5</b> Strengths</a></li>
<li class="chapter" data-level="7.6" data-path="likelihoodism.html"><a href="likelihoodism.html#limitations-3"><i class="fa fa-check"></i><b>7.6</b> Limitations</a></li>
<li class="chapter" data-level="7.7" data-path="likelihoodism.html"><a href="likelihoodism.html#resources"><i class="fa fa-check"></i><b>7.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>8</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="causal-inference.html"><a href="causal-inference.html#causality-for-the-rescue"><i class="fa fa-check"></i><b>8.1</b> Causality for the Rescue</a></li>
<li class="chapter" data-level="8.2" data-path="causal-inference.html"><a href="causal-inference.html#causality"><i class="fa fa-check"></i><b>8.2</b> Causality</a></li>
<li class="chapter" data-level="8.3" data-path="causal-inference.html"><a href="causal-inference.html#the-causal-mindset"><i class="fa fa-check"></i><b>8.3</b> The Causal Mindset</a></li>
<li class="chapter" data-level="8.4" data-path="causal-inference.html"><a href="causal-inference.html#directed-acyclic-graph-dag"><i class="fa fa-check"></i><b>8.4</b> Directed Acyclic Graph (DAG)</a></li>
<li class="chapter" data-level="8.5" data-path="causal-inference.html"><a href="causal-inference.html#many-frameworks-for-causality"><i class="fa fa-check"></i><b>8.5</b> Many Frameworks For Causality</a></li>
<li class="chapter" data-level="8.6" data-path="causal-inference.html"><a href="causal-inference.html#from-causal-model-to-statistical-estimator"><i class="fa fa-check"></i><b>8.6</b> From Causal Model to Statistical Estimator</a></li>
<li class="chapter" data-level="8.7" data-path="causal-inference.html"><a href="causal-inference.html#strengths-4"><i class="fa fa-check"></i><b>8.7</b> Strengths</a></li>
<li class="chapter" data-level="8.8" data-path="causal-inference.html"><a href="causal-inference.html#limitations-4"><i class="fa fa-check"></i><b>8.8</b> Limitations</a></li>
<li class="chapter" data-level="8.9" data-path="causal-inference.html"><a href="causal-inference.html#further-reading"><i class="fa fa-check"></i><b>8.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>9</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="machine-learning.html"><a href="machine-learning.html#one-or-many-mindsets"><i class="fa fa-check"></i><b>9.1</b> One or Many Mindsets?</a></li>
<li class="chapter" data-level="9.2" data-path="machine-learning.html"><a href="machine-learning.html#computer-oriented-task-driven-and-externally-motivated"><i class="fa fa-check"></i><b>9.2</b> Computer-Oriented, Task-Driven and Externally Motivated</a></li>
<li class="chapter" data-level="9.3" data-path="machine-learning.html"><a href="machine-learning.html#strengths-5"><i class="fa fa-check"></i><b>9.3</b> Strengths</a></li>
<li class="chapter" data-level="9.4" data-path="machine-learning.html"><a href="machine-learning.html#limitations-5"><i class="fa fa-check"></i><b>9.4</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised-ml.html"><a href="supervised-ml.html"><i class="fa fa-check"></i><b>10</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="supervised-ml.html"><a href="supervised-ml.html#competing-with-the-wrong-mindset"><i class="fa fa-check"></i><b>10.1</b> Competing With the Wrong Mindset</a></li>
<li class="chapter" data-level="10.2" data-path="supervised-ml.html"><a href="supervised-ml.html#predict-everything"><i class="fa fa-check"></i><b>10.2</b> Predict Everything</a></li>
<li class="chapter" data-level="10.3" data-path="supervised-ml.html"><a href="supervised-ml.html#supervised-machine-learning"><i class="fa fa-check"></i><b>10.3</b> Supervised Machine Learning</a></li>
<li class="chapter" data-level="10.4" data-path="supervised-ml.html"><a href="supervised-ml.html#learning-is-searching"><i class="fa fa-check"></i><b>10.4</b> Learning Is Searching</a></li>
<li class="chapter" data-level="10.5" data-path="supervised-ml.html"><a href="supervised-ml.html#overfitting"><i class="fa fa-check"></i><b>10.5</b> Overfitting</a></li>
<li class="chapter" data-level="10.6" data-path="supervised-ml.html"><a href="supervised-ml.html#evaluation"><i class="fa fa-check"></i><b>10.6</b> Evaluation</a></li>
<li class="chapter" data-level="10.7" data-path="supervised-ml.html"><a href="supervised-ml.html#an-automatable-mindset"><i class="fa fa-check"></i><b>10.7</b> An Automatable Mindset</a></li>
<li class="chapter" data-level="10.8" data-path="supervised-ml.html"><a href="supervised-ml.html#a-competitive-mindset"><i class="fa fa-check"></i><b>10.8</b> A Competitive Mindset</a></li>
<li class="chapter" data-level="10.9" data-path="supervised-ml.html"><a href="supervised-ml.html#nature-statistics-and-supervised-learning"><i class="fa fa-check"></i><b>10.9</b> Nature, Statistics and Supervised Learning</a></li>
<li class="chapter" data-level="10.10" data-path="supervised-ml.html"><a href="supervised-ml.html#strengths-6"><i class="fa fa-check"></i><b>10.10</b> Strengths</a></li>
<li class="chapter" data-level="10.11" data-path="supervised-ml.html"><a href="supervised-ml.html#limitations-6"><i class="fa fa-check"></i><b>10.11</b> Limitations</a></li>
<li class="chapter" data-level="10.12" data-path="supervised-ml.html"><a href="supervised-ml.html#references"><i class="fa fa-check"></i><b>10.12</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#what-type-of-traveler-are-you"><i class="fa fa-check"></i><b>11.1</b> What Type of Traveler Are You?</a></li>
<li class="chapter" data-level="11.2" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#the-unsupervised-learning-mindset"><i class="fa fa-check"></i><b>11.2</b> The Unsupervised Learning Mindset</a></li>
<li class="chapter" data-level="11.3" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#many-tasks"><i class="fa fa-check"></i><b>11.3</b> Many Tasks</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#clustering-and-outlier-detection"><i class="fa fa-check"></i><b>11.3.1</b> Clustering and Outlier Detection</a></li>
<li class="chapter" data-level="11.3.2" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#anomaly-detection"><i class="fa fa-check"></i><b>11.3.2</b> Anomaly Detection</a></li>
<li class="chapter" data-level="11.3.3" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#association-rule-learning"><i class="fa fa-check"></i><b>11.3.3</b> Association Rule Learning</a></li>
<li class="chapter" data-level="11.3.4" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#dimensionality-reduction"><i class="fa fa-check"></i><b>11.3.4</b> Dimensionality Reduction</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#strengths-7"><i class="fa fa-check"></i><b>11.4</b> Strengths</a></li>
<li class="chapter" data-level="11.5" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#limitations-7"><i class="fa fa-check"></i><b>11.5</b> Limitations</a></li>
<li class="chapter" data-level="11.6" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#resources-1"><i class="fa fa-check"></i><b>11.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>12</b> Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#the-reinforcement-learning-mindset"><i class="fa fa-check"></i><b>12.1</b> The Reinforcement Learning Mindset</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#reward-and-value"><i class="fa fa-check"></i><b>12.1.1</b> Reward and Value</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#what-to-learn"><i class="fa fa-check"></i><b>12.2</b> What to Learn</a></li>
<li class="chapter" data-level="12.3" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#the-other-mindsets"><i class="fa fa-check"></i><b>12.3</b> The Other Mindsets</a></li>
<li class="chapter" data-level="12.4" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#strengths-8"><i class="fa fa-check"></i><b>12.4</b> Strengths</a></li>
<li class="chapter" data-level="12.5" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#limitations-8"><i class="fa fa-check"></i><b>12.5</b> Limitations</a></li>
<li class="chapter" data-level="12.6" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#references-1"><i class="fa fa-check"></i><b>12.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>13</b> Deep Learning</a></li>
<li class="chapter" data-level="14" data-path="interpretable-ml.html"><a href="interpretable-ml.html"><i class="fa fa-check"></i><b>14</b> Interpretable Machine Learning</a></li>
<li class="chapter" data-level="15" data-path="design-based-inference.html"><a href="design-based-inference.html"><i class="fa fa-check"></i><b>15</b> Design-based Inference</a></li>
<li class="chapter" data-level="" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li> 
<li><a href="https://christophmolnar.com/impressum/" target="_blank">Impressum</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling Mindsets</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">

<a id="cta-button-desktop" href="https://leanpub.com/modeling-mindsets" rel="noopener noreferrer" target="blank"> Buy Book </a>

<a id="cta-button-device" href="https://leanpub.com/modeling-mindsets" rel="noopener noreferrer" target="blank">Buy</a>
  

<div id="frequentist-inference" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Frequentist Inference<a href="frequentist-inference.html#frequentist-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<ul>
<li>Popular modeling mindset in science.</li>
<li>The world consists of probability distributions with fixed parameters that have to be uncovered.</li>
<li>Interprets probability as long-run relative frequencies from which hypothesis tests, confidence intervals and p-values are derived.</li>
<li>A statistical mindset, with <a href="bayesian-inference.html#bayesian-inference">Bayesian inference</a> and <a href="likelihoodism.html#likelihoodism">likelihoodism</a> as alternatives.</li>
</ul>
<!-- TODOs:

-->
<!-- CONTENT TO ADD 
*  Write about difficulty of writing this chapter: I grew up with frequentist mindset, so it was an act to frame what is really special about this mindset. like growing up in a city and being asked what is special about this city: you have to know other cities well to do it. you then have to question what you take for granted.
*  Power of statistical tests
* Lindley Paradox https://en.wikipedia.org/wiki/Lindley%27s_paradox
* https://en.wikipedia.org/wiki/Foundations_of_statistics#Comparisons_of_characteristics
-->
<!-- example of frequentist study -->
<!-- source of study:  https://academic.oup.com/aje/article/154/8/748/131397 -->
<p>Drinking alcohol is associated with a higher risk of diabetes in middle-aged men.
At least this is what a study claims.<span class="citation"><sup><a href="#ref-kao2001alcohol" role="doc-biblioref">4</a></sup></span></p>
<p>The study modeled type II diabetes as a function of various risk factors.
The researchers found out that alcohol significantly increases the diabetes risk for middle-aged men by a factor of <span class="math inline">\(1.81\)</span>.</p>
<!-- familiar frequentist terms -->
<p>“Significant” and “associated with” are familiar terms when reading about scientific results.
The researchers in the study used a popular modeling mindset to draw conclusions from the data: frequentist inference.
There is no particular reason why I chose this study other than it is not exceptional.
When someone thinks in significance levels, p-values, hypothesis tests, null hypotheses, and confidence intervals, they are probably frequentist.</p>
<!-- dominant mindset -->
<p>In many scientific fields, such as medicine and psychology, frequentist inference is the dominant mindset.
All frequentist papers follow similar patterns, make similar assumptions, and contain similar tables and figures.
Knowing how to interpret model coefficients, confidence intervals and p-values is like a key to contemporary scientific progress.
Or at least a good part of it.
Frequentism not only dominates science, but has a firm foothold in industry as well:
Statisticians, data scientists, and whatever the role will be called in the future, use frequentist inference to create value for businesses:
From analyzing A/B tests for a website to calculating portfolio risk to monitoring quality on production lines.</p>
<!-- frequentist criticized -->
<p>As much as frequentism dominates the world of data, it’s also criticized.
Frequentist inference has been the analysis method for scientific “findings” that turned out to be a waste of research time.
You may have heard about the replication crisis.<span class="citation"><sup><a href="#ref-ioannidis2005most" role="doc-biblioref">5</a></sup></span>
Many scientific findings in psychology, medicine, social sciences and other fields could not be replicated.
The problem with that is that replication is at the center of the scientific method.
Many causes have contributed to the replication crisis
But frequentist statistics is right in the middle of it.
The frequentist mindset enables practices such as multiple testing and p-hacking.
Mix this with the pressure on academics to “publish or perish”.
The result is a community that is incentivized to squeeze out “significant” results at a high rate.
Frequentism is a decision-focused mindset and can give seemingly simple yes/no answers.
Humans are lazy.
So we tend to forget all the footnotes and remarks that come with the model.</p>
<!-- frequentism is statistical model -->
<p>Frequentist inference is a statistical modeling mindset:
It depends on random variables, probability distributions, and statistical models.
But as mentioned in the chapter <a href="statistical-modeling.html#statistical-modeling">Statistical Modeling</a>, these ingredients are not sufficient to make statements about the world.</p>
<!-- probability interpretation -->
<p>Frequentism comes with a specific interpretation of probability:
Probability is seen as the relative frequency of an event in infinitely repeated trials.
That’s why it’s called frequentism: frequentist inference emphasizes the (relative) frequency of events.
But how do these long-run frequencies help to gain insights from the model?</p>
<!-- wine example continued -->
<p>Let’s go back to the <span class="math inline">\(1.81\)</span> increase in diabetes risk among men who drink a lot of alcohol.
<span class="math inline">\(1.81\)</span> is larger than <span class="math inline">\(1\)</span>, so there seems to be a difference between men who drink alcohol and the ones who don’t.
But how can the researchers be sure that the <span class="math inline">\(1.81\)</span> is not a random result?
<!-- dice example -->
For fair dice, the average eyes in the long-run series of experiments is 3.5.
If I roll a die 10 times and the average is 4, would you say it’s an unfair die?
No? Would you say it’s unfair if the average is 4.5? 5? Or if a 6 shows up 10 times?</p>
<!-- uncertainty in the wine example -->
<p>The researchers applied frequentist thinking to decide between randomness and true effects.
The parameter of interest is a coefficient in a logistic regression model.
The logistic regression model links variables such as alcohol to diabetes.
In the diabetes study, a 95% confidence interval for the alcohol coefficient was reported:
The interval goes from <span class="math inline">\(1.14\)</span> to <span class="math inline">\(2.92\)</span>.
This interval settles the question of randomness versus signal:
The interval doesn’t contain <span class="math inline">\(1\)</span>, and so the researchers concluded that alcohol is a risk factor for diabetes (in men).
This confidence interval describes uncertainty regarding the alcohol coefficient.
If we were to repeat the analysis many times with new samples, the respective 95% confidence interval would cover the “true” parameter 95% of the time.
Always under the condition that the model assumptions were correct.</p>
<div id="frequentist-probability" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Frequentist probability<a href="frequentist-inference.html#frequentist-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- frequentist interpretation of example -->
<p>The interpretation of the confidence interval reveals the <strong>core philosophy of frequentism</strong>:</p>
<ul>
<li>The world can be described by probability distributions;</li>
<li>The parameters of the probability distributions are constant and unknown;</li>
<li>Repeated measurements/experiments reveal the true parameter values in the long-run.</li>
</ul>
<p>In contrast, <a href="bayesian-inference.html#bayesian-inference">Bayesianism</a> assumes that the parameters of the distributions are themselves random variables.
As the frequentists collect more and more data (<span class="math inline">\(n \to \infty\)</span>), their parameter estimation gets closer and closer to the true parameter (if the estimator is unbiased).
With each additional data point, the uncertainty of the estimated parameter shrinks and the confidence interval becomes narrower.</p>
<!-- other examples -->
<p>The frequentist interpretation of probability requires imagination.
Frequentists start with a population in mind.
The population can be adults between 20 and 29 living in Iceland, daily measurements of water quality of the Nile River, or 1-inch wood screws manufactured in a factory in the U.S. state of Texas.
These populations can be described by finding out their probability distributions.
Going back to the initial example:
What’s the probability that a middle-aged man will develop diabetes in the next 12 months?
Frequentists would say: There is an unknown and fixed probability for diabetes.
The more people we observe, the more accurate our estimate of the probability of diabetes becomes.
We estimate the probability of diabetes as the relative frequency of diabetes in the population.
Probabilities are frequencies in the long-run:</p>
<p><span class="math display">\[P(X=1) = \lim_{n \mapsto \infty} \frac{1}{n}\sum_{i=1}^{n} I(x_i = 1)\]</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dice"></span>
<img src="figures/dice-1.png" alt="The line shows how the relative frequency of 6 eyes changes as the number of dice roles increases from 1 to 100 (left to right)." width="\textwidth" />
<p class="caption">
FIGURE 5.1: The line shows how the relative frequency of 6 eyes changes as the number of dice roles increases from 1 to 100 (left to right).
</p>
</div>
<!-- imagined repetition of the experiment -->
<!-- TODO: Move further down? -->
<p>Imagining experiments that will never take place is essential to the frequentist mindset.
By defining probability in terms of long-run frequencies, frequentism requires imagining that the sampling and experiment will be done many times.
These “imagined” experiments are central to the interpretation of confidence intervals, p-values, and hypothesis tests.
And <em>every</em> interpretation of probability in the frequentist mindset has to be connected to frequencies of events in long-run samples / experiments.</p>
<!-- likelihood principle violated -->
<p>These imagined experiments have a curious implication for frequentism.
Frequentism violates the likelihood principle, which says that all evidence about the data is contained in the likelihood function.
But with frequentism, it’s important to know what experiments we are further imagining.
You can find a simple example involving coin tosses in the chapter on <a href="likelihoodism.html#likelihoodism">Likelihoodism</a>.
<a href="likelihoodism.html#likelihoodism">Likelihoodism</a> and <a href="#bayesianism-inference">Bayesianism</a> adhere to the likelihood principle.</p>
</div>
<div id="estimators-are-random-variables" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Estimators are Random Variables<a href="frequentist-inference.html#estimators-are-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- estimands are fixed -->
<p>We can learn a lot about frequentist inference, especially in contrast to Bayesian inference, by understanding which “things” are random variables and which are not.
In the frequentist mindset, the estimand, the “true” but unknown parameter is assumed to be fixed.
Mean, variance and other distribution parameters, model coefficients, nuisance parameters, all are seen as having some unknown but fixed value.
And the values can be uncovered with frequentist inference.
Bayesians, in contrast, view all these parameters as random variables.</p>
<!-- estimators are random variables -->
<p>Since the quantities of interest are seen as fixed but unknown, the frequentist’s job is to estimate them from data.
The estimation is done with a statistical estimator: A mathematical procedure for inferring the estimand from data.
The estimator is a function of the data.
And data are realizations of random variables.
As a consequence, the estimators themselves become random variables.
Let’s compare this with the Bayesian mindset:
Bayesians assume that the parameters are random variables.
Bayesian inference updates the (prior) probability distributions of the parameters, which results in the posterior distributions of the parameters.</p>
<!-- other parameters -->
<p>Typical frequentist constructs like confidence intervals, test statistics and p-values are also random variables.
Mix this with the long-run frequencies and you get a special interpretation, for example, for <a href="frequentist-inference.html#confidence-intervals">confidence intervals</a>.</p>
<!-- example -->
<p>Let’s say you want to know how many teas you drink on average per day.
If you are a frequentist, you would assume that there is a true but unknown daily number of teas.
Let’s call this estimand <span class="math inline">\(\lambda\)</span>.
The frequentist might assume that the daily number of teas follows a Poisson distribution.
The Poisson distribution can handle count data well, and is described by the “intensity” <span class="math inline">\(\lambda\)</span> with which events happen.
The intensity parameter <span class="math inline">\(\lambda\)</span> is also the expected number of events.
Teas in our case.
We could estimate the tea intensity using the maximum likelihood estimator: <span class="math inline">\(\hat{\lambda}= \frac{1}{n} \sum_{i=1}^n k_i\)</span>, where <span class="math inline">\(k_i\)</span> is the number of teas on day <span class="math inline">\(i\)</span>.
Our estimator <span class="math inline">\(\hat{\lambda}\)</span> is a random variable.
If the model assumptions are correct and if the world is truly frequentist, then the estimator <span class="math inline">\(\hat{\lambda}\)</span> will get closer and closer to the true <span class="math inline">\(\lambda\)</span> as <span class="math inline">\(n\)</span> increases.
The estimator <span class="math inline">\(\hat{\lambda}\)</span> approximately follows a Normal distribution.</p>
<!-- connecting estimators with the world -->
<p>Frequentist inference builds on the fact that the estimators are random variables.
Combined with the idea of fixed true parameters, it becomes possible to connect the analysis results to the world.
A commonly used tool to derive insights about the world is null hypothesis significance testing.</p>
</div>
<div id="null-hypothesis-significance-testing" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Null Hypothesis Significance Testing<a href="frequentist-inference.html#null-hypothesis-significance-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- example of decision under frequentism -->
<p>Let’s say, your estimator <span class="math inline">\(\hat{\lambda}\)</span> says that you drink 2.5 teas per day on average.
Initially you had the hypothesis that you drink at least 3.0 teas per day.
Obviously, <span class="math inline">\(2.5 \neq 3.0\)</span>, so the initial hypothesis is incorrect.
Case closed.
But that would be too simple an answer, wouldn’t it?
You also wouldn’t say that a coin is unfair if heads come up in 51/100 tosses just because <span class="math inline">\(51 \neq 50\)</span>.
But when would a frequentist reject the initial hypothesis of 3.0 teas?
Would we reject the hypothesis if we get <span class="math inline">\(\hat{\lambda}&lt;2.9\)</span>, or <span class="math inline">\(\hat{\lambda}&lt;2.5\)</span> or maybe must it be much lower, like <span class="math inline">\(\hat{\lambda}&lt;1.5\)</span>?
With the <a href="statistical-modeling.html#statistical-modeling">statistical modeling mindset</a> alone, we can’t answer this question.</p>
<!-- frequentist answer: NHST -->
<p>The frequentist mindset has an answer to this question of whether to accept or reject a hypothesis.
The frequentist estimator for the number of teas is a random variable that is supposed to approximate the true number of teas.
We can make (frequentist) probabilistic statements about this estimator.
And while the true value for <span class="math inline">\(\lambda\)</span> is unknown, we can study the hypothesis of <span class="math inline">\(\lambda = 3.0\)</span> by examining the random variable <span class="math inline">\(\hat{\lambda}\)</span>.</p>
<!-- central idea of frequentism -->
<p>This idea of proposing a hypothesis, and then accepting or rejecting it based on a statistical model or test is called null hypothesis significance testing. <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>
Hypothesis testing is a central method in the frequentist modeling mindset.
Hypothesis tests simplify decisions:
The frequentist accepts or rejects the so-called null hypothesis based on the results of the statistical model.
A statistical model can be very simple:
It can be as simple as assuming that the data follow a Normal distribution and comparing two means with a Student t-test.</p>
<!-- NHST example -->
<p>How does a hypothesis test work?</p>
<ul>
<li>Start with a hypothesis.</li>
<li>Formulate the <strong>alternative or null hypothesis</strong>.</li>
<li>Decide which statistical test to use. This step includes modeling the data. In fact, all statistical tests are statistical models<span class="citation"><sup><a href="#ref-mcelreath2020statistical" role="doc-biblioref">3</a></sup></span></li>
<li>Calculate the distribution of the parameter estimates under the null hypothesis (or rather, the test statistic <span class="math inline">\(T\)</span>).</li>
<li>Choose the significance level <span class="math inline">\(\alpha\)</span>: the probability threshold at which to reject the null hypothesis assuming it would be true. Often <span class="math inline">\(\alpha = 0.05\)</span>.</li>
<li>Calculate the p-value: Assume that the null hypothesis is correct. Then p is the probability of getting a more extreme test statistic <span class="math inline">\(T\)</span> than was actually observed. See figure <a href="frequentist-inference.html#fig:hypothesis">5.2</a>.</li>
<li>If <span class="math inline">\(\text{p-value} &lt;\alpha\)</span>, then the null hypothesis is rejected.</li>
</ul>
<p>Some examples of tests and test statistics:</p>
<ul>
<li>Comparing the means of two distributions. Do Germans consume more pretzels than U.S. Americans? Hypothesis: Germans eat more pretzels. The “model” of the data simply assumes a Normal distribution for average pretzel consumption per person and year. The null hypothesis would be that Germans and U.S. Americans consume the same amount. Then we would run a t-test. The test statistic in the t-test is the (scaled) difference of the two means.</li>
<li>Estimating the effect of one variable on another. Is surgery better than physiotherapy for treating a torn meniscus in your knee? The statistical model could be a linear regression model. The model could predict knee pain dependent on whether a patient had physiotherapy or surgery. The null hypothesis would be that there is no difference in pain, so a model coefficient of zero for surgery/physiotherapy. The test statistic <span class="math inline">\(T\)</span> would be the coefficient divided by its standard deviation.</li>
</ul>
<!-- the p-value -->
<p>The p-value has a frequentist interpretation because it’s based on long-run frequencies.
To interpret the p-value, we have to pretend that the null hypothesis is true.
Then the p-value is the probability of observing the outcome of our analysis or a more extreme one.
Again, the frequentist interprets probability with imagined future experiments.
A p-value of 0.03 for an estimated average of 3.0 daily teas would mean the following:
If we repeat the analysis many times and the null hypothesis <span class="math inline">\(\lambda = 2.5\)</span> is correct, 3% of the time we would observe an estimate of <span class="math inline">\(\hat{\lambda} \geq 3\)</span>.
If <span class="math inline">\(\alpha = 0.05\)</span> was chosen, the null hypothesis would be rejected.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hypothesis"></span>
<img src="figures/hypothesis-1.png" alt="Frequentists make binary decisions based on hypothesis tests. Assuming the null distribution, the null hypothesis is rejected if the observed estimand is extreme." width="\textwidth" />
<p class="caption">
FIGURE 5.2: Frequentists make binary decisions based on hypothesis tests. Assuming the null distribution, the null hypothesis is rejected if the observed estimand is extreme.
</p>
</div>
<!-- Weirdness of H_0 -->
<p>Null hypothesis testing is very weird.
It’s like answering the question around two corners.
Let’s say a researcher wants to prove that a drug prevents migraines.
They test the drug because they expect it to work, so the hypothesis they assume to be true is that patients that take the drug have fewer migraines.
But the null hypothesis is formulated the other way around:
The null hypothesis assumes that the drug has no effect.
Suddenly the goal of the researcher becomes to show that the null hypothesis is false, rather than showing that their hypothesis is correct.
The problem with statistical models is: We can’t prove that they are true because our assumptions are not testable.
With frequentist inference, however, we can tell how likely a model result is under a given hypothesis and given the data.
That’s why hypothesis tests work by rejection.
The <a href="likelihoodism.html#likelihoodism">likelihoodist mindset</a> navigates this issue:
two statistical models are compared in terms of the evidence through the likelihood.</p>
<p>Null hypothesis tests are even more troublesome.</p>
<ul>
<li>The choice of the null hypothesis is arbitrary.</li>
<li>If the null hypothesis is accepted, it’s not evidence that it’s true. It just means that the data that were observed are not in conflict with the null hypothesis. But there is still an infinite number of models that could have produced the same results.</li>
<li>If the null hypothesis is rejected, it doesn’t mean that the hypothesis of interest is true.</li>
<li>A significant result doesn’t mean that the deviation from the null hypothesis is relevant. The drug has a significant effect on the disease progression? The difference might be too small to be relevant.</li>
<li>Especially the larger the data sample, the more likely the null hypothesis is rejected, because the tiniest differences to the null hypothesis are enough to produce significant results when <span class="math inline">\(n\)</span> becomes large.</li>
</ul>
</div>
<div id="confidence-intervals" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Confidence Intervals<a href="frequentist-inference.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- Alternative, but equivalent: Confidence interval -->
<p>Frequentists use confidence intervals as an alternative to statistical tests.
Hypothesis tests and confidence intervals ultimately lead to the same decisions, but confidence intervals are more informative. <!-- citation needed -->
Many statisticians prefer confidence intervals over mere p-values. <!-- citation needed --></p>
<!-- CI ingredients -->
<p>Remember that estimators, such as model parameters, are random variables?
That means that estimators have probability distributions.
A confidence interval describes where the mass of that distribution lies.
The interval consists of the estimator, and the lower and upper bounds for the mass of the distribution.
The modeler decides the percentage of the distribution in the confidence interval through the <span class="math inline">\(\alpha\)</span>-level.
If <span class="math inline">\(\alpha = 0.05\)</span>, then we get a 95%-confidence interval.
The construction of the confidence interval depends on the probability distribution we have derived for the quantity of interest (coefficient, mean estimate, …).</p>
<!-- Interpretation of confidence intervals -->
<p>How are the confidence intervals to be interpreted?
Well, in a frequentist manner, of course!
The “true” parameter value is fixed, so it’s not a random variable.
To say that the true parameter is in the confidence interval with a 95% probability would be false.
The true parameter is either in the interval or it’s not, we just don’t know.
The confidence itself is a random variable since it’s derived from data and therefore from other random variables.
So the interpretation of a 95% confidence interval is:
If we were to repeat the analysis many times, the confidence interval would cover the true value of the quantity of interest 95% of the time.
Only given that the model assumptions are correct.
As you can see, this is a very frequentist point of view: the confidence interval is interpreted in the context of repeated experiments.</p>
<!-- Compared CI to tests -->
<!-- TODO: Remove? -->
<!--
Confidence intervals look different from tests at first glance.
But there is an equivalence.
A confidence interval can be translated into a hypothesis tests that give the same accept/reject result for the same significance level $\alpha$.
But the confidence interval contains more information than a simple hypothesis test.
The interval gives the estimate, and lower and upper bounds.
This gives the modeler a better sense of the estimate and its uncertainty.
Rejection of the null hypothesis is equivalent to the null hypothesis being outside of the confidence interval.

-->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ci"></span>
<img src="figures/ci-1.png" alt="100 95\% confidence intervals and the true value." width="\textwidth" />
<p class="caption">
FIGURE 5.3: 100 95% confidence intervals and the true value.
</p>
</div>
</div>
<div id="strengths-1" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Strengths<a href="frequentist-inference.html#strengths-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Once you understand frequentist inference, you have the key to understanding most modern research findings. I studied statistics and can now quickly grasp many research papers. For example, to figure out whether I should have knee surgery for my torn meniscus, I read papers comparing knee surgery and physiotherapy alone. All of those papers used frequentist methods, and although I didn’t understand everything, I was able to quickly get an idea of their analyses and results.</li>
<li>Frequentist methods are generally faster to compute than methods from <a href="bayesian-inference.html#bayesian-inference">Bayesian inference</a> or <a href="supervised-ml.html#supervised-ml">machine learning</a>.</li>
<li>Compared to <a href="bayesian-inference.html#bayesian-inference">Bayesianism</a>, no prior information about the parameters is required. This makes frequentism more objective.</li>
<li>Frequentism allows binary decisions (accept/reject hypothesis). This simplicity is one of the reasons why frequentism is popular for both scientific publications and business decisions.</li>
<li>Frequentism has all advantages of <a href="statistical-modeling.html#statistical-modeling">statistical models</a> in general: a solid theoretical foundation and an appreciation of the data-generating process.</li>
<li>When the underlying process is a long-run, repeated experiment, frequentist inference shines. Casino games, model benchmarks, …</li>
<li>The scientific method requires that scientific experiments be repeatable. The frequentist idea that truth lies in long-run frequencies of experiments is therefore well compatible with a core idea of science.</li>
</ul>
</div>
<div id="limitations-1" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Limitations<a href="frequentist-inference.html#limitations-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Frequentism makes it easy to <strong>over-simplify questions</strong> into yes/no-questions. Reducing models to binary decisions obscures critical model assumptions and the difficult trade-offs that had to be made for the analysis.</li>
<li>Focusing on p-values encourages <strong>p-hacking</strong>: the either conscious or unconscious search for “positive” results. Guided by the lure of a significant result, researchers and data scientists may play around with their analysis until the p-value in question is small enough. With <span class="math inline">\(\alpha\)</span>-level of 0.05, 1 in 20 null hypotheses are falsely rejected. P-hacking increases this percentage of false positive findings.</li>
<li>Similarly, if the analysis is exploratory rather than hypothesis-driven, a naive frequentist approach may produce many false positive findings. Look again at figure <a href="frequentist-inference.html#fig:ci">5.3</a>: Imagine these were confidence intervals for different variables. Again, for <span class="math inline">\(\alpha = 0.05\)</span>, we would expect 1 in 20 hypothesis tests to yield false positives. Now imagine a data scientist testing hundreds of hypothesis tests. This problem is called the multiple testing problem. There are solutions, but they are not always used and multiple testing can be very subtle.</li>
<li>The frequentist interpretation of probability is very awkward when it comes to confidence intervals and p-values. They are commonly misinterpreted. Arguably, frequentist confidence intervals are not what practitioners want. <a href="bayesian-inference.html#bayesian-inference">Bayesian</a> credibility intervals are more aligned with the natural interpretation of uncertainty.</li>
<li>Frequentist analysis depends not only on the data, but also on the experimental design. This is a violation of the likelihood principle that says that all information about the data must be contained in the likelihood, see also the example in the <a href="likelihoodism.html#likelihoodism">Likelihoodism</a> chapter.</li>
<li>Frequentist probability can fail in the simplest scenarios: Imagine you are modeling the probability of rain in August. The data only has 20 August days, all of which are without rain. The frequentist answer is that there is absolutely no chance that it will ever rain in August. The frequentist recommendation is that to collect more data if we want a better answer. <a href="bayesian-inference.html#bayesian-inference">Bayesianism</a> offers a solution to involve prior information for such estimates.</li>
<li>There is an “off-the-shelf”-mentality among users of frequentist inference. Instead of carefully adapting a probability model to the data, an off-the-shelf statistical test or statistical model is chosen. The choice is based on just a few properties of the data. For example, there are popular flow charts of choosing an appropriate statistical test. <!-- https://bookdown.org/content/4857/generalized-linear-madness.html --> <!-- citation needed --></li>
<li>Frequentist statistics says nothing about causality except that “correlation does not imply causation”.</li>
<li>Weird interpretation of probability: Often it does not make any sense to interpret every probability with imagined experiments. For example, the probability for a party to get the majority vote in an election requires to imagine multiple elections, yet under the same circumstances, like same year, same candidates, and so on.</li>
</ul>

</div>
</div>
<h3>References<a href="references-2.html#references-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-mcelreath2020statistical" class="csl-entry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">McElreath R. Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC; 2020. </div>
</div>
<div id="ref-kao2001alcohol" class="csl-entry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Kao WL, Puddey IB, Boland LL, Watson RL, Brancati FL. Alcohol consumption and the risk of type 2 diabetes mellitus: Atherosclerosis risk in communities study. American journal of epidemiology. 2001;154(8):748–57. </div>
</div>
<div id="ref-ioannidis2005most" class="csl-entry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Ioannidis JP. Why most published research findings are false. PLoS medicine. 2005;2(8):e124. </div>
</div>
<div id="ref-perezgonzalez2015fisher" class="csl-entry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Perezgonzalez JD. Fisher, neyman-pearson or NHST? A tutorial for teaching data testing. Frontiers in psychology. 2015;223. </div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>There are two other “main” approaches for hypothesis testing: The Fisher approach, and the Neyman-Pearson approach. Null hypothesis significance testing is a combination of the two.<span class="citation"><sup><a href="#ref-perezgonzalez2015fisher" role="doc-biblioref">6</a></sup></span><a href="frequentist-inference.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-modeling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/modeling-mindsets/edit/master/manuscript/frequentism.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
