<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Supervised Learning | Modeling Mindsets</title>
  <meta name="description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Supervised Learning | Modeling Mindsets" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Supervised Learning | Modeling Mindsets" />
  
  <meta name="twitter:description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2022-06-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machine-learning.html"/>
<link rel="next" href="unsupervised-ml.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>

<style>

#cta-button-desktop:hover, #cta-button-device:hover {
  background-color:   #ffc266; 
  border-color:   #ffc266; 
  box-shadow: none;
}
#cta-button-desktop, #cta-button-device{
  color: white;
  background-color:  #ffa31a;
  text-shadow:1px 1px 0 #444;
  text-decoration: none;
  border: 2px solid  #ffa31a;
  border-radius: 10px;
  position: fixed;
  padding: 5px 10px;
  z-index: 10;
  }

#cta-button-device {
  box-shadow: 0px 10px 10px -5px rgba(194,180,190,1);
  display:none;
  right: 20px;
  bottom: 20px;
  font-size: 20px;
 }

#cta-button-desktop {
  box-shadow: 0px 20px 20px -10px rgba(194,180,190,1);
  display:display;
  padding: 8px 16px;
  right: 40px;
  bottom: 40px;
  font-size: 25px;
}

@media (max-width : 450px) {
  #cta-button-device {display:block;}
  #cta-button-desktop {display:none;}
}


</style>





<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Mindsets</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="what-this-book-is-about.html"><a href="what-this-book-is-about.html"><i class="fa fa-check"></i><b>1</b> What This Book is About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="what-this-book-is-about.html"><a href="what-this-book-is-about.html#who-this-book-is-for"><i class="fa fa-check"></i><b>1.1</b> Who This Book is For</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Models</a></li>
<li class="chapter" data-level="3" data-path="mindsets.html"><a href="mindsets.html"><i class="fa fa-check"></i><b>3</b> Mindsets</a></li>
<li class="chapter" data-level="4" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>4</b> Statistical Modeling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#random-variables"><i class="fa fa-check"></i><b>4.1</b> Random Variables</a></li>
<li class="chapter" data-level="4.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#probability-distributions"><i class="fa fa-check"></i><b>4.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="4.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#assuming-a-distribution"><i class="fa fa-check"></i><b>4.3</b> Assuming a Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-model"><i class="fa fa-check"></i><b>4.4</b> Statistical Model</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i>Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-hypothesis"><i class="fa fa-check"></i>Statistical Hypothesis</a></li>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#interpreting-parameters"><i class="fa fa-check"></i>Interpreting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="statistical-modeling.html"><a href="statistical-modeling.html#joint-or-conditional-distribution"><i class="fa fa-check"></i><b>4.5</b> Joint or Conditional Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="statistical-modeling.html"><a href="statistical-modeling.html#regression-models"><i class="fa fa-check"></i><b>4.6</b> Regression Models</a></li>
<li class="chapter" data-level="4.7" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-evaluation"><i class="fa fa-check"></i><b>4.7</b> Model Evaluation</a></li>
<li class="chapter" data-level="4.8" data-path="statistical-modeling.html"><a href="statistical-modeling.html#data-generating-process-dgp"><i class="fa fa-check"></i><b>4.8</b> Data-Generating Process (DGP)</a></li>
<li class="chapter" data-level="4.9" data-path="statistical-modeling.html"><a href="statistical-modeling.html#drawing-conclusions-about-the-world"><i class="fa fa-check"></i><b>4.9</b> Drawing Conclusions About the World</a></li>
<li class="chapter" data-level="4.10" data-path="statistical-modeling.html"><a href="statistical-modeling.html#strengths"><i class="fa fa-check"></i><b>4.10</b> Strengths</a></li>
<li class="chapter" data-level="4.11" data-path="statistical-modeling.html"><a href="statistical-modeling.html#limitations"><i class="fa fa-check"></i><b>4.11</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="frequentist-inference.html"><a href="frequentist-inference.html"><i class="fa fa-check"></i><b>5</b> Frequentist Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="frequentist-inference.html"><a href="frequentist-inference.html#frequentist-probability"><i class="fa fa-check"></i><b>5.1</b> Frequentist probability</a></li>
<li class="chapter" data-level="5.2" data-path="frequentist-inference.html"><a href="frequentist-inference.html#estimators-are-random-variables"><i class="fa fa-check"></i><b>5.2</b> Estimators are Random Variables</a></li>
<li class="chapter" data-level="5.3" data-path="frequentist-inference.html"><a href="frequentist-inference.html#null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>5.3</b> Null Hypothesis Significance Testing</a></li>
<li class="chapter" data-level="5.4" data-path="frequentist-inference.html"><a href="frequentist-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.4</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.5" data-path="frequentist-inference.html"><a href="frequentist-inference.html#strengths-1"><i class="fa fa-check"></i><b>5.5</b> Strengths</a></li>
<li class="chapter" data-level="5.6" data-path="frequentist-inference.html"><a href="frequentist-inference.html#limitations-1"><i class="fa fa-check"></i><b>5.6</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>6</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayes-theorem"><i class="fa fa-check"></i><b>6.1</b> Bayes Theorem</a></li>
<li class="chapter" data-level="6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prior-probability"><i class="fa fa-check"></i><b>6.2</b> Prior Probability</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#picking-a-prior"><i class="fa fa-check"></i>Picking a Prior</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#likelihood"><i class="fa fa-check"></i><b>6.3</b> Likelihood</a></li>
<li class="chapter" data-level="6.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#evidence"><i class="fa fa-check"></i><b>6.4</b> Evidence</a></li>
<li class="chapter" data-level="6.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-probability-estimation"><i class="fa fa-check"></i><b>6.5</b> Posterior Probability Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#sample-from-the-posterior-with-mcmc"><i class="fa fa-check"></i>Sample From the Posterior with MCMC</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summarizing-the-posterior-samples"><i class="fa fa-check"></i><b>6.6</b> Summarizing the Posterior Samples</a></li>
<li class="chapter" data-level="6.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#from-model-to-world"><i class="fa fa-check"></i><b>6.7</b> From Model to World</a></li>
<li class="chapter" data-level="6.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#simulate-to-predict"><i class="fa fa-check"></i><b>6.8</b> Simulate to Predict</a></li>
<li class="chapter" data-level="6.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#strengths-2"><i class="fa fa-check"></i><b>6.9</b> Strengths</a></li>
<li class="chapter" data-level="6.10" data-path="bayesian-inference.html"><a href="bayesian-inference.html#limitations-2"><i class="fa fa-check"></i><b>6.10</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="likelihoodism.html"><a href="likelihoodism.html"><i class="fa fa-check"></i><b>7</b> Likelihoodism</a>
<ul>
<li class="chapter" data-level="7.1" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-principle"><i class="fa fa-check"></i><b>7.1</b> Likelihood Principle</a></li>
<li class="chapter" data-level="7.2" data-path="likelihoodism.html"><a href="likelihoodism.html#law-of-likelihood"><i class="fa fa-check"></i><b>7.2</b> Law of Likelihood</a></li>
<li class="chapter" data-level="7.3" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-intervals"><i class="fa fa-check"></i><b>7.3</b> Likelihood Intervals</a></li>
<li class="chapter" data-level="7.4" data-path="likelihoodism.html"><a href="likelihoodism.html#why-frequentism-violates-the-likelihood-principle"><i class="fa fa-check"></i><b>7.4</b> Why Frequentism Violates the Likelihood Principle</a></li>
<li class="chapter" data-level="7.5" data-path="likelihoodism.html"><a href="likelihoodism.html#strengths-3"><i class="fa fa-check"></i><b>7.5</b> Strengths</a></li>
<li class="chapter" data-level="7.6" data-path="likelihoodism.html"><a href="likelihoodism.html#limitations-3"><i class="fa fa-check"></i><b>7.6</b> Limitations</a></li>
<li class="chapter" data-level="7.7" data-path="likelihoodism.html"><a href="likelihoodism.html#resources"><i class="fa fa-check"></i><b>7.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>8</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="causal-inference.html"><a href="causal-inference.html#does-the-drug-help"><i class="fa fa-check"></i><b>8.1</b> Does The Drug Help?</a></li>
<li class="chapter" data-level="8.2" data-path="causal-inference.html"><a href="causal-inference.html#causality"><i class="fa fa-check"></i><b>8.2</b> Causality</a></li>
<li class="chapter" data-level="8.3" data-path="causal-inference.html"><a href="causal-inference.html#the-causal-mindset"><i class="fa fa-check"></i><b>8.3</b> The Causal Mindset</a></li>
<li class="chapter" data-level="8.4" data-path="causal-inference.html"><a href="causal-inference.html#directed-acyclic-graph"><i class="fa fa-check"></i><b>8.4</b> Directed Acyclic Graph</a></li>
<li class="chapter" data-level="8.5" data-path="causal-inference.html"><a href="causal-inference.html#many-frameworks-for-causality"><i class="fa fa-check"></i><b>8.5</b> Many Frameworks For Causality</a></li>
<li class="chapter" data-level="8.6" data-path="causal-inference.html"><a href="causal-inference.html#from-causal-model-to-statistical-estimator"><i class="fa fa-check"></i><b>8.6</b> From Causal Model to Statistical Estimator</a></li>
<li class="chapter" data-level="8.7" data-path="causal-inference.html"><a href="causal-inference.html#strengths-4"><i class="fa fa-check"></i><b>8.7</b> Strengths</a></li>
<li class="chapter" data-level="8.8" data-path="causal-inference.html"><a href="causal-inference.html#limitations-4"><i class="fa fa-check"></i><b>8.8</b> Limitations</a></li>
<li class="chapter" data-level="8.9" data-path="causal-inference.html"><a href="causal-inference.html#further-reading"><i class="fa fa-check"></i><b>8.9</b> Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>9</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="machine-learning.html"><a href="machine-learning.html#one-or-many-mindsets"><i class="fa fa-check"></i><b>9.1</b> One or Many Mindsets?</a></li>
<li class="chapter" data-level="9.2" data-path="machine-learning.html"><a href="machine-learning.html#computer-oriented-task-driven-and-externally-motivated"><i class="fa fa-check"></i><b>9.2</b> Computer-Oriented, Task-Driven and Externally Motivated</a></li>
<li class="chapter" data-level="9.3" data-path="machine-learning.html"><a href="machine-learning.html#strengths-5"><i class="fa fa-check"></i><b>9.3</b> Strengths</a></li>
<li class="chapter" data-level="9.4" data-path="machine-learning.html"><a href="machine-learning.html#limitations-5"><i class="fa fa-check"></i><b>9.4</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervised-ml.html"><a href="supervised-ml.html"><i class="fa fa-check"></i><b>10</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="supervised-ml.html"><a href="supervised-ml.html#competing-with-the-wrong-mindset"><i class="fa fa-check"></i><b>10.1</b> Competing With the Wrong Mindset</a></li>
<li class="chapter" data-level="10.2" data-path="supervised-ml.html"><a href="supervised-ml.html#predict-everything"><i class="fa fa-check"></i><b>10.2</b> Predict Everything</a></li>
<li class="chapter" data-level="10.3" data-path="supervised-ml.html"><a href="supervised-ml.html#supervised-machine-learning"><i class="fa fa-check"></i><b>10.3</b> Supervised Machine Learning</a></li>
<li class="chapter" data-level="10.4" data-path="supervised-ml.html"><a href="supervised-ml.html#learning-is-searching"><i class="fa fa-check"></i><b>10.4</b> Learning Is Searching</a></li>
<li class="chapter" data-level="10.5" data-path="supervised-ml.html"><a href="supervised-ml.html#overfitting"><i class="fa fa-check"></i><b>10.5</b> Overfitting</a></li>
<li class="chapter" data-level="10.6" data-path="supervised-ml.html"><a href="supervised-ml.html#evaluation"><i class="fa fa-check"></i><b>10.6</b> Evaluation</a></li>
<li class="chapter" data-level="10.7" data-path="supervised-ml.html"><a href="supervised-ml.html#an-automatable-mindset"><i class="fa fa-check"></i><b>10.7</b> An Automatable Mindset</a></li>
<li class="chapter" data-level="10.8" data-path="supervised-ml.html"><a href="supervised-ml.html#a-competitive-mindset"><i class="fa fa-check"></i><b>10.8</b> A Competitive Mindset</a></li>
<li class="chapter" data-level="10.9" data-path="supervised-ml.html"><a href="supervised-ml.html#nature-statistics-and-supervised-learning"><i class="fa fa-check"></i><b>10.9</b> Nature, Statistics and Supervised Learning</a></li>
<li class="chapter" data-level="10.10" data-path="supervised-ml.html"><a href="supervised-ml.html#strengths-6"><i class="fa fa-check"></i><b>10.10</b> Strengths</a></li>
<li class="chapter" data-level="10.11" data-path="supervised-ml.html"><a href="supervised-ml.html#limitations-6"><i class="fa fa-check"></i><b>10.11</b> Limitations</a></li>
<li class="chapter" data-level="10.12" data-path="supervised-ml.html"><a href="supervised-ml.html#references"><i class="fa fa-check"></i><b>10.12</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html"><i class="fa fa-check"></i><b>11</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#what-type-of-traveler-are-you"><i class="fa fa-check"></i><b>11.1</b> What Type of Traveler Are You?</a></li>
<li class="chapter" data-level="11.2" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#the-unsupervised-learning-mindset"><i class="fa fa-check"></i><b>11.2</b> The Unsupervised Learning Mindset</a></li>
<li class="chapter" data-level="11.3" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#many-tasks"><i class="fa fa-check"></i><b>11.3</b> Many Tasks</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#clustering-and-outlier-detection"><i class="fa fa-check"></i><b>11.3.1</b> Clustering and Outlier Detection</a></li>
<li class="chapter" data-level="11.3.2" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#anomaly-detection"><i class="fa fa-check"></i><b>11.3.2</b> Anomaly Detection</a></li>
<li class="chapter" data-level="11.3.3" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#association-rule-learning"><i class="fa fa-check"></i><b>11.3.3</b> Association Rule Learning</a></li>
<li class="chapter" data-level="11.3.4" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#dimensionality-reduction"><i class="fa fa-check"></i><b>11.3.4</b> Dimensionality Reduction</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#strengths-7"><i class="fa fa-check"></i><b>11.4</b> Strengths</a></li>
<li class="chapter" data-level="11.5" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#limitations-7"><i class="fa fa-check"></i><b>11.5</b> Limitations</a></li>
<li class="chapter" data-level="11.6" data-path="unsupervised-ml.html"><a href="unsupervised-ml.html#resources-1"><i class="fa fa-check"></i><b>11.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>12</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="13" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>13</b> Deep Learning</a></li>
<li class="chapter" data-level="14" data-path="interpretable-ml.html"><a href="interpretable-ml.html"><i class="fa fa-check"></i><b>14</b> Interpretable Machine Learning</a></li>
<li class="chapter" data-level="15" data-path="design-based-inference.html"><a href="design-based-inference.html"><i class="fa fa-check"></i><b>15</b> Design-based Inference</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li> 
<li><a href="https://christophmolnar.com/impressum/" target="_blank">Impressum</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling Mindsets</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">

<a id="cta-button-desktop" href="https://leanpub.com/modeling-mindsets" rel="noopener noreferrer" target="blank"> Buy Book </a>

<a id="cta-button-device" href="https://leanpub.com/modeling-mindsets" rel="noopener noreferrer" target="blank">Buy</a>
  

<div id="supervised-ml" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Supervised Learning<a href="supervised-ml.html#supervised-ml" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- TODO

* mention semi-supervised learning?
* Write about global and local maxima?
* Write more about how the supervised mindset influences current discourses and reality
  * election forecasting
  * surveillance (e.g. flagging suspicious conversations by Microsoft)
  * diagnosis as binary decisions
  * however: often lacks causality, so decision made in the light of prediction might create havoc
* Read and cite "The forecast trap"
  *  
-->
<ul>
<li>Prediction-focused mindset that invites automation and competition.</li>
<li>A good model has low generalization error - it predicts unseen data well.</li>
<li>A <a href="machine-learning.html#machine-learning">machine learning</a> mindset.</li>
</ul>
<div id="competing-with-the-wrong-mindset" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Competing With the Wrong Mindset<a href="supervised-ml.html#competing-with-the-wrong-mindset" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- Motivation: short-coming of stats mindset in ML competition -->
<p>It was 2012, and I had just fitted a statistical model to predict whether a patient would develop type 2 diabetes given some risk factors.
And now it was time to test the model.
You see, I wasn’t the only one modeling diabetes:
I was competing with many other data scientists.
I uploaded the CSV-file with the prediction results to the competition website.
A table with two columns: One with the patient identifier and one with the probability for that patient to develop diabetes.
One row per patient.
Fingers crossed.
But then came the disappointing results.
The predictions of my model sucked.
What had happened?</p>
<!-- my background going into competition -->
<p>At the time, I was a master’s student in statistics.
I modeled diabetes risk using a generalized additive model, a model often used in statistical modeling.
Most importantly, I created the model coming from a frequentist modeling mindset.
So I thought a lot about the data-generating process, manually added or removed variables, and evaluated the model based on goodness of fit on the training data.
The statistical modeling mindset failed me in this prediction competition.
And that what confused me at first.
After all, statistical models can be used for prediction and classification, and the same statistical models are also used in machine learning.
Heck! Statistical learning is even one of the foundations of machine learning!
This overlap of theory and methods may mislead one to believe that statistical modeling and supervised machine learning are interchangeable.
<!-- statistical mindset bad for ML competition -->
But the (archetypal) modeling mindsets are fundamentally different, especially the idea of what makes a good model and how evaluation works.
For me, the disappointing model performance was a catalyst for understanding the supervised machine learning mindset.
For the diabetes competition, I began to seriously study machine learning models like boosting and random forests, but also how to properly evaluate the performance of machine learning models.
While I didn’t win any money in the competition (59th place out of 145), I did win something more valuable:
With supervised machine learning, I gained a new modeling mindset.</p>
</div>
<div id="predict-everything" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Predict Everything<a href="supervised-ml.html#predict-everything" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In supervised machine learning, everything is a prediction task.
<!-- Definition of Prediction -->
Before complaints come rolling in, here is my definition of prediction:
The proposition of values that are unknown at a given time, but for which a ground truth exists or will exist.
Assigning data points to a cluster is not prediction because there is no ground truth for the clusters.
Prediction can mean assigning a classification score, a numerical value (regression), a survival time, etc.<br />
It’s amazing how many applications can be formulated as prediction tasks:</p>
<ul>
<li>Credit score can be expressed as the probability that someone will repay their loan. Based on information about the person’s financial situation, a predictive model assigns a score that indicates how likely it is that the person will pay back the money.</li>
<li>Predictive maintenance: Many machines require regular inspection and repair. Supervised machine learning models can be used to predict when machines might fail based on current conditions.</li>
<li>Demand forecasting: using historical sales data to estimate demand for a product.</li>
<li>Image classification: how should the image be classified? For example, image classification can be used to detect cancer on CT images.</li>
</ul>
<p>As these examples show, supervised machine learning adopts the “task-oriented” trait of the machine learning mindset.
Prediction is a task and can be used to do practical things.
A modeling mindset that deals only with prediction tasks seems very narrow.
But there is a surprisingly large number of applications for which prediction can be useful.
And the type of data that can be used in predictive models can also be quite diverse:
The input to the predictive model, usually called features, can be text, an image, a time series, a DNA sequence, a video, a good old Excel spreadsheet, …</p>
</div>
<div id="supervised-machine-learning" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Supervised Machine Learning<a href="supervised-ml.html#supervised-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- risk minimization -->
<p>Turning any modeling task into a prediction problem is not the only defining trait of the supervised machine learning mindset.
A core idea of supervised machine learning is risk minimization.
And a good supervised model has a low generalization error, meaning that the prediction for new data points is close to the respective ground truth.
To quantify how close a prediction is to the ground truth, the machine learner uses a loss function <span class="math inline">\(L(y, f(x))\)</span>.
The loss function <span class="math inline">\(L\)</span> takes the ground truth value <span class="math inline">\(y\)</span> and the predicted value <span class="math inline">\(\hat{y}\)</span> and returns a number.
The larger the number, the worse the prediction.
In the diabetes example, <span class="math inline">\(y\)</span> could be 1 for diabetes and 0 for healthy.
Accordingly, <span class="math inline">\(f(x)\)</span> could be the predicted diabetes probability between 0 and 1.</p>
<p>The goal in supervised machine learning is now to find the function <span class="math inline">\(f\)</span> that minimizes the loss across the data:</p>
<p><span class="math display">\[\arg \min_{f} L(y, x, f(y))\]</span></p>
<!-- externally motivated -->
<p>The focus here is on optimizing the loss, and there are no specific constraints on what the function <span class="math inline">\(f\)</span> may look like.
In statistical modeling, <span class="math inline">\(f\)</span> would have to be motivated based on probability distributions, but in machine learning, any form is allowed.
This makes supervised learning a true machine learning mindset:
The modeling approach is externally motivated by how the model predictions performs on new data.
The model is trained using one part of the data (training data) and evaluated on another part (test data).</p>
<!-- why own mindset -->
<p>Is there enough to the claim that supervised machine learning is its own mindset?
I believe that supervised machine learning is mindset of its own.
The reason sounds very “bureaucratic”, but it has strong implications on the mindset: supervised machine learning ALWAYS requires a ground truth.
That’s also what separates supervised from unsupervised and reinforcement learning.
We want the model to predict diabetes? For the training data, we actually need to know if a patients have diabetes.
The model is supposed to predict machine failure? We need a data set where we have actually observed many machines, some of which have also failed at some point.</p>
<!-- own mindset socially -->
<p>The archetypal supervised learner wouldn’t even consider working on unsupervised learning.
For example, I know many machine learning researchers who work exclusively with supervised machine learning.
There is no ground truth, so what the heck should the model “predict” anyway?
And even if we defined something that the model should “predict”, without ground truth we wouldn’t really know how to evaluate it properly.
My observation is that in industry people are more pragmatic and it would be harder to find a pure supervised machine learning because there are many problems without labels.</p>
</div>
<div id="learning-is-searching" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Learning Is Searching<a href="supervised-ml.html#learning-is-searching" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- what does f look like? -->
<p>We have danced around the question of what the function <span class="math inline">\(f\)</span> is; the function that maps from the features <span class="math inline">\(x\)</span> to the desired values <span class="math inline">\(y\)</span>.
Without any restrictions on the form of <span class="math inline">\(f\)</span>, finding the best or at least a good <span class="math inline">\(f\)</span> can seem infeasible.
In statistical modeling it’s “simple”:
We can derive estimators for <span class="math inline">\(f\)</span> from the theoretical distributions.
This makes the search space much smaller, and searching <span class="math inline">\(f\)</span> is simplified to finding the best parameterization of a statistical model.
In cases such as the linear regression model, we can even be sure that we have the optimal parameterization.
In supervised machine learning, the loss <span class="math inline">\(L\)</span> helps us evaluate the <span class="math inline">\(f\)</span>’s, but it does not tell us how to search for it.</p>
<!-- where f's live -->
<p>We have to go where the functions <span class="math inline">\(f\)</span> live:
This would be the hypothesis space.
It’s a big space.
I mean, the space has to hold infinitely many functions, even if you have only one feature from which to predict the target <span class="math inline">\(y\)</span>.
In order to search within this space, we have to at least put some constraints on what <span class="math inline">\(f\)</span> might look like.
And that’s where all the different model classes come into play: <strong>decision trees, support vector machines, linear regression models, random forests, boosting, neural networks</strong>, …</p>
<!-- simplifying the search -->
<p>For simplicity, let’s say we have only one feature <span class="math inline">\(x_1\)</span> and want to predict <span class="math inline">\(y\)</span> from it.
The prediction function would then be <span class="math inline">\(f(x_1)\)</span>.
If we restrict <span class="math inline">\(f\)</span> to be a linear model, we only have to search all <span class="math inline">\(f\)</span>’s of the form <span class="math inline">\(f(x_1) = \beta_0 + \beta_1 x_1\)</span>.
We have just simplified the search in the vast hypothesis space to the search for the optimal parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
A much simpler task.
<!-- simplifying the search part II -->
Similarly, all other machine learning algorithms make the hypothesis space manageable so that it can be searched.
Think of the hypothesis space as a dark forest.
Machine learning algorithms illuminate paths through the forest so that we can search for the best <span class="math inline">\(f\)</span> within the these paths.
The globally best <span class="math inline">\(f\)</span> might not be within this illuminated path, so we will usually only find a locally optimal <span class="math inline">\(f\)</span>.
Machine learning algorithms differ in the form and complexity they allow for <span class="math inline">\(f\)</span>.
Decision tree algorithms produce <span class="math inline">\(f\)</span>’s that look like step functions, since most trees algorithms only allow discrete jumps in prediction.
Neural network are universal function approximators that can, in theory, approximate any function <span class="math inline">\(f\)</span>.<span class="citation"><sup><a href="#ref-hornik1989multilayer" role="doc-biblioref">18</a></sup></span></p>
<!-- optimization procedures -->
<p>Each machine learning algorithm has its own procedure to search the hypothesis space.
Most of the time, this search is about finding the right parameters for a model:
Neural networks use gradient descent with backpropagation to adjust the weights, regression models use maximum likelihood to find the ideal values for the coefficients, and so on.</p>
</div>
<div id="overfitting" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Overfitting<a href="supervised-ml.html#overfitting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- Overfitting -->
<p>Supervised machine learning has one major nemesis: overfitting.
Remember, the goal is to achieve a low generalization error.
But as long as we only use training data, we don’t know how well the model will perform with new data.
Worse, machine learning models can easily overfit the training data.
Think of overfitting as memorization of the training data.
When the model perfectly memorizes the training data, it will have have zero loss on the training data, but will likely perform badly with new data.</p>
<!-- Underfitting -->
<p>The opposite of overfitting is underfitting.
If the hypothesis space is too constrained, then model may not be flexible enough to represent the true relationship between the input features and the target.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:underoverfitting"></span>
<img src="figures/underoverfitting-1.png" alt="The target y is dependent on x through a function f(x) (dotted line). The observed data (points) have an additional random error. One model overfits the randomness in the training data (black line), and the other underfits (grey line)." width="\textwidth" />
<p class="caption">
FIGURE 10.1: The target y is dependent on x through a function f(x) (dotted line). The observed data (points) have an additional random error. One model overfits the randomness in the training data (black line), and the other underfits (grey line).
</p>
</div>
<p>Fitting a supervised model means walking a fine line between underfitting and overfitting.
Model evaluation is central to finding this delicate balance and not ending up on one side of the cliff or the other.</p>
</div>
<div id="evaluation" class="section level2 hasAnchor" number="10.6">
<h2><span class="header-section-number">10.6</span> Evaluation<a href="supervised-ml.html#evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- cooking contest -->
<p>Let’s say you want to enter a cooking competition.
A contest with a panel of judges who will evaluate your food and insult you on live TV if it tastes like crap.
You’ve been practicing your cooking skills for a while.
Fortunately, you have some “ground truth data” about how well your food is received.
You cook for family and friends often, and they’ve given you feedback on how good your dishes tasted.
Over time, your dishes got better and better, and today you consistently get excellent reviews from family and friends.</p>
<!-- how to evaluate cooking contest -->
<p>The jury is the ultimate test of your cooking skills.
You have never cooked for these judges before.
So this test is about how well your cooking skills generalize to new data points.
But are your confident enough about your skills?
What if your supposed kitchen prowess is attuned to strange tastes?
Your your family might be addicted to salt, for example.
And the jurors would be like: “Did you cook this with seawater?”, “What is this? Bread? Or a salt lick for goats?”.
In order not to bring shame to your family and name, you decide to validate your skills before this ultimate test.
So you cook for some new people who have never tried your dishes before.
This way you can evaluate your skills without having to waste your shot in the contest.</p>
<!-- test data -->
<p>Rigorous evaluation is close to the heart of supervised machine learners.
A model generalizes well to the real world if the generalization error is low.
A typical recommendation of supervised machine learners is to set up the evaluation pipeline even before training the first model.
In supervised machine learning, evaluation means measuring a loss <span class="math inline">\(L\)</span> for unseen data, usually called “test data”.
The test data is like the judges in a cooking competition.
The machine learner may not to use the test data to train the model or test it prematurely.
The test data may only be used for the final evaluation.
If the test data influences the model training or choice in any way, it’s “burned” and does not show the true performance of the model.
Rather the evaluation will be too optimistic.</p>
<!-- validation data -->
<p>Because of this “burning” of the test data, machine learners need different strategy to guide their model building.
The test data are set aside.
Whether to compare models or to try different configurations of a model, the machine learner needs unseen data.
The trick is to repeat this train/test split within the training data.
So we cut off a portion of the training data that can be used to evaluate modeling decisions.
This data set is usually referred to as validation data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:evaluation"></span>
<img src="figures/evaluation-1.png" alt="For evaluation, the data is usually split into training, validation and test data. There are more complex splitting schemes where the data is split multiple times." width="\textwidth" />
<p class="caption">
FIGURE 10.2: For evaluation, the data is usually split into training, validation and test data. There are more complex splitting schemes where the data is split multiple times.
</p>
</div>
<!-- many ways for splits -->
<p>In the simplest version, the data is split once before model training into training, validation and test data.
In reality, techniques such as cross-validation are used to split the data multiple times and reuse the data intelligently.</p>
<!-- unseen data focus 
What deserves attention at this point is the emphasis on unseen data.
This emphasis distinguishes supervised machine learning from the other mindsets.
In the statistical modeling mindset models are usually evaluated terms of goodness-of-fit, often on training data itself, and diagnostic plots.
A particular trade of supervised machine learning is the almost exclusive focus on the generalization error as the selection criterion for models.
This focus, as a consequence, encourages automation and competition -- with far reaching consequences for the supervised machine learning mindset.

Interestingly, the evaluation of machine learning algorithms has a [frequentist](#frequentism) character, and is best approached with a frequentist mindset.
Each evaluation of a model can be seen as an experiment.
If repeated, but with a different sample of the data, we have exactly the condition of a repeated experiment.
-->
</div>
<div id="an-automatable-mindset" class="section level2 hasAnchor" number="10.7">
<h2><span class="header-section-number">10.7</span> An Automatable Mindset<a href="supervised-ml.html#an-automatable-mindset" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- Invites Automation  -->
<p>Supervised machine learning is automatable to a degree that surpasses all other mindsets.
Using a well-defined evaluation procedure, the generalization error, the entire process of model building can be automated.
Supervised machine learning is essentially an optimization algorithm.
In statistical modeling, such as Bayesian and frequentist inference, we have to make all the assumptions, choose the right distributions, decide on the variables to use in the model, look at diagnostic plots, …</p>
<!-- AutoML -->
<p>There is an entire subfield of machine learning, AutoML, that deals with automating the entire training pipeline.
This can include feature engineering, model training, hyperparameter optimization, evaluation, etc.
Automating the supervised machine learning pipeline is computationally intensive, so there is a lot of research on how to automate everything in a smart way.
As a result of this automation capability, there is an entire industry with hundreds of web services and products that automate the training process for you.</p>
<!-- AutoML is also bad -->
<p>But automation is also problematic.
It creates distance between the modelers and the underlying modeling task.
Automation makes modelers less aware of the shortcomings of the data.
On paper, the model may look very good, because the generalization error is small.
But under the surface, the model may be a garbage because it uses features that are not available at the time of the prediction, or the data are terribly biased, or missing data were not handled correctly to, name just a few possible errors.</p>
</div>
<div id="a-competitive-mindset" class="section level2 hasAnchor" number="10.8">
<h2><span class="header-section-number">10.8</span> A Competitive Mindset<a href="supervised-ml.html#a-competitive-mindset" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- kaggle and competition -->
<p>Another consequence of the one-dimensional evaluation is that supervised learning is a competitive mindset.
Modeling becomes a sport: which is best model for a task?
It also invites competition between people.
Entire websites are dedicated to hosting machine learning competitions where the best modelers can win money.
Sometimes a lot of money.
Your skills as a modeler are reduced to your ability to optimize a single metric.
That metric puts you on the leaderboard, which ranks modelers.
A ranking that ignores many things, such as domain expertise, model interpretability, coding skills, runtime, …
The idea of competition has also taken hold of machine learning research itself.
Scientific progress, in large parts, has become a sport.
Progress in machine learning research is when a new machine learning algorithm beats other algorithms in benchmarks.</p>
</div>
<div id="nature-statistics-and-supervised-learning" class="section level2 hasAnchor" number="10.9">
<h2><span class="header-section-number">10.9</span> Nature, Statistics and Supervised Learning<a href="supervised-ml.html#nature-statistics-and-supervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we have seen, the mindsets of statistical modeling and supervised machine learning can be quite different.
At their core, the two mindsets involve different ideas of how to model aspects of the world.
The following comparison is more or less a summary of Leo Breiman’s famous article “Statistical Modeling: The Two Cultures”.<span class="citation"><sup><a href="#ref-breiman2001statistical" role="doc-biblioref">19</a></sup></span></p>
<p>In the context of prediction, we can think of nature as a mechanism that takes features <span class="math inline">\(X\)</span> and produces output <span class="math inline">\(Y\)</span>.
This mechanism is unknown and we want to learn about it using models.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nature"></span>
<img src="figures/nature-1.png" alt="Nature" width="\textwidth" />
<p class="caption">
FIGURE 10.3: Nature
</p>
</div>
<p>Statistical modelers fill this box with a statistical model.
The statistical model is supposed to represent nature.
It is supposed to reproduce the inner workings of nature.
If we are somewhat convinced that we have found the mechanism, we can then take the model parameters and interpret them as if it was the true mechanism in nature.
Nature’s true mechanism is unknown and not fully specified by the data, we have to make some assumptions about the forms of this mechanism, which we represent with the function <span class="math inline">\(f\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stats"></span>
<img src="figures/stats-1.png" alt="Statistical Model" width="\textwidth" />
<p class="caption">
FIGURE 10.4: Statistical Model
</p>
</div>
<p>In supervised machine learning, nature is seen as unknowable, or at least no attempt is even made to reverse-engineer the inner mechanisms of nature.
Instead of the intrinsic approach, supervised machine learning takes an extrinsic approach.
The supervised model is supposed to mimic nature.
It should show the same behaviour as nature, but it doesn’t matter if it achieves this behaviour in the same way.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:supervised"></span>
<img src="figures/supervised-1.png" alt="Supervised Machine Learning Model" width="\textwidth" />
<p class="caption">
FIGURE 10.5: Supervised Machine Learning Model
</p>
</div>
<p>Again, a cooking analogy:
Suppose you want to recreate a dish that you ate in a restaurant.
A statistician cook would try to find a plausible recipe, even if the end result is not perfect.
The supervised machine learner cook would only be interested in the end result;
it doesn’t matter whether it’s was exactly the same recipe.</p>
<p>No one mindset is inherently better or more useful than another.
They are different mindsets with different strengths and limitations.
If a task involves evaluating unseen data against a well-defined performance metric, the best approach to that task is probably supervised machine learning.
If your task requires a model with a strong theory that can explain the relationships in the data, statistical modeling is the way to go.</p>
</div>
<div id="strengths-6" class="section level2 hasAnchor" number="10.10">
<h2><span class="header-section-number">10.10</span> Strengths<a href="supervised-ml.html#strengths-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>The most straightforward mindset when it comes to making predictions.</li>
<li>Loss function <span class="math inline">\(L\)</span> allows the model to be adapted quite well to the task at hand.</li>
<li>Supervised machine learning is highly automatable.</li>
<li>Supervised learning has a very coherent evaluation approach that I personally find very convincing, though quite one-sided. Measuring how well the model predicts new data is a very compelling way to define a good model.</li>
</ul>
</div>
<div id="limitations-6" class="section level2 hasAnchor" number="10.11">
<h2><span class="header-section-number">10.11</span> Limitations<a href="supervised-ml.html#limitations-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Supervised learning without constraints does not lead to interpretable models and is therefore not as well suited to for gaining insights.</li>
<li>Supervised learning is not as theoretically sound as statistical modeling.</li>
<li>Making decisions based on only the most likely outcome ignores tail risks from less likely, but possible extreme outcomes.</li>
<li>Uncertainty quantification is not a first class citizen as it is in, for example, <a href="#bayesian">Bayesian inference</a>. The modeler has to rely on a subset of machine learning algorithms that quantify uncertainty (for example Gaussian processes) or they have to use additional tools such as conformal prediction.</li>
<li>Automation can lead to overlooking issues with the data and the task formulation.</li>
<li>Generalization error is a good way to quantify generalization, relying solely on this metric will fail in the dumbest ways. There are many examples, such as using asthma as a predictor of lower risk of pneumonia<span class="citation"><sup><a href="#ref-caruana2015intelligible" role="doc-biblioref">20</a></sup></span>, classifying based on watermarks<span class="citation"><sup><a href="#ref-lapuschkin2019unmasking" role="doc-biblioref">21</a></sup></span>, and misclassifying dogs as wolfs based on snow in the background<span class="citation"><sup><a href="#ref-ribeiro2016should" role="doc-biblioref">22</a></sup></span>.</li>
<li>Feedback loops can break the models. Deployed into the wild, supervised learning models influences and even creates data that might end up training a future version of the same model. But this feedback loop is not well understood and difficult to respect in the modeling process.</li>
</ul>
</div>
<div id="references" class="section level2 hasAnchor" number="10.12">
<h2><span class="header-section-number">10.12</span> References<a href="supervised-ml.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Statistical Modeling: The Two Cultures by Leo Breiman.<span class="citation"><sup><a href="#ref-breiman2001statistical" role="doc-biblioref">19</a></sup></span> Highly recommended to understand differences between statistical modeling and supervised machine learning.</li>
<li>I can recommend the book “Elements of Statistical Learning”<span class="citation"><sup><a href="#ref-hastie2009elements" role="doc-biblioref">23</a></sup></span> which covers not only supervised learning but also other machine learning topics. The book has a strong influence from the statistical modeling mindset.</li>
</ul>

</div>
</div>
<h3>References<a href="references-1.html#references-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-hornik1989multilayer" class="csl-entry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Hornik K, Stinchcombe M, White H. Multilayer feedforward networks are universal approximators. Neural networks. 1989;2(5):359–66. </div>
</div>
<div id="ref-breiman2001statistical" class="csl-entry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Breiman L. Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science. 2001;16(3):199–231. </div>
</div>
<div id="ref-caruana2015intelligible" class="csl-entry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Caruana R, Lou Y, Gehrke J, Koch P, Sturm M, Elhadad N. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In: Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 2015. p. 1721–30. </div>
</div>
<div id="ref-lapuschkin2019unmasking" class="csl-entry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Lapuschkin S, Wäldchen S, Binder A, Montavon G, Samek W, Müller KR. Unmasking clever hans predictors and assessing what machines really learn. Nature communications. 2019;10(1):1–8. </div>
</div>
<div id="ref-ribeiro2016should" class="csl-entry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Ribeiro MT, Singh S, Guestrin C. " why should i trust you?" Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 2016. p. 1135–44. </div>
</div>
<div id="ref-hastie2009elements" class="csl-entry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Hastie T, Tibshirani R, Friedman JH, Friedman JH. The elements of statistical learning: Data mining, inference, and prediction. Vol. 2. Springer; 2009. </div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machine-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised-ml.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/modeling-mindsets/edit/master/manuscript/supervised-ml.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
